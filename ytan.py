import streamlit as st
import os
import glob
import json
import time
import re
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import google.oauth2.credentials
import googleapiclient.discovery
import google.auth.transport.requests
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
from datetime import datetime, timedelta

# region [1. ì„¤ì • ë° ìƒìˆ˜ (Config & Constants)]
# ==========================================
# ê¸°ë³¸ í˜ì´ì§€ ì„¤ì • ë° ë””ìì¸, ìƒìˆ˜ ì •ì˜
# ==========================================
st.set_page_config(
    page_title="Drama YouTube Insight", 
    page_icon="ğŸ“Š",
    layout="wide", 
    initial_sidebar_state="expanded"
)

# UI ë””ìì¸ CSS
custom_css = """
    <style>
        /* 1. í—¤ë” íˆ¬ëª…í™” ë° ë¶ˆí•„ìš” ìš”ì†Œ ìˆ¨ê¹€ (ì‚¬ì´ë“œë°” ë²„íŠ¼ ìœ ì§€) */
        header[data-testid="stHeader"] { background: transparent; }
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        [data-testid="stDecoration"] {display: none;}
        
        /* 2. ë©”ì¸ ì»¨í…ì¸  ì—¬ë°± ì¡°ì • */
        .block-container { padding-top: 1rem; padding-bottom: 3rem; }

        /* 3. ì•± ë°°ê²½ ì„¤ì • */
        .stApp { background-color: #f8f9fa; }

        /* 4. ì¹´ë“œ ë° ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ */
        div[data-testid="stMetric"] {
            background-color: white;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #eee;
            box-shadow: 0 2px 4px rgba(0,0,0,0.02);
            text-align: center;
        }
        div[data-testid="stMetricLabel"] { font-size: 0.9rem; color: #6c757d; }
        div[data-testid="stMetricValue"] { font-size: 1.6rem; font-weight: 700; color: #2d3436; }

        [data-testid="stForm"] {
            background-color: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            border: 1px solid #e0e0e0;
            padding: 20px;
        }
        
        h1, h2, h3, h4 { color: #2d3436; font-weight: 700; }
        .stDataFrame { border: 1px solid #f0f0f0; border-radius: 8px; }
    </style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

MAX_WORKERS = 7
SCOPES = [
    'https://www.googleapis.com/auth/yt-analytics.readonly',
    'https://www.googleapis.com/auth/youtube.readonly'
]
DEFAULT_LIMIT_DATE = "2025-01-01"

# [ì§€ë„ìš©] ì£¼ìš” êµ­ê°€ ISO-2 -> ISO-3 ë§¤í•‘
ISO_MAPPING = {
    'KR': 'KOR', 'US': 'USA', 'JP': 'JPN', 'VN': 'VNM', 'TH': 'THA', 
    'ID': 'IDN', 'TW': 'TWN', 'PH': 'PHL', 'MY': 'MYS', 'IN': 'IND',
    'BR': 'BRA', 'MX': 'MEX', 'RU': 'RUS', 'GB': 'GBR', 'DE': 'DEU',
    'FR': 'FRA', 'CA': 'CAN', 'AU': 'AUS', 'HK': 'HKG', 'SG': 'SGP'
}
# endregion


# region [2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utilities)]
# ==========================================
# í…ìŠ¤íŠ¸ ì •ì œ, ìˆ«ì í¬ë§·íŒ…, ë‚ ì§œ ë³€í™˜ ë“± í—¬í¼ í•¨ìˆ˜
# ==========================================
def normalize_text(text):
    if not text: return ""
    return re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', text).lower()

def format_korean_number(num):
    """ìˆ«ìë¥¼ '1ì–µ 2345ë§Œ 6789íšŒ' í˜•íƒœë¡œ ë³€í™˜"""
    if num == 0: return "0íšŒ"
    s = ""
    if num >= 100000000:
        eok = num // 100000000
        rem = num % 100000000
        s += f"{int(eok)}ì–µ "
        num = rem
    if num >= 10000:
        man = num // 10000
        rem = num % 10000
        s += f"{int(man)}ë§Œ "
        num = rem
    if num > 0:
        s += f"{int(num)}"
    return s.strip() + "íšŒ"

TRAFFIC_MAP = {
    'YT_SEARCH': 'ìœ íŠœë¸Œ ê²€ìƒ‰', 'RELATED_VIDEO': 'ì¶”ì²œ ë™ì˜ìƒ',
    'BROWSE_FEATURES': 'íƒìƒ‰ ê¸°ëŠ¥', 'EXT_URL': 'ì™¸ë¶€ ë§í¬',
    'NO_LINK_OTHER': 'ê¸°íƒ€', 'PLAYLIST': 'ì¬ìƒëª©ë¡',
    'VIDEO_CARD': 'ì¹´ë“œ/ìµœì¢…í™”ë©´', 'NOTIFICATION': 'ì•Œë¦¼'
}

def map_traffic_source(key):
    return TRAFFIC_MAP.get(key, key)

def parse_utc_to_kst_date(utc_str):
    try:
        dt_utc = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%SZ")
        dt_kst = dt_utc + timedelta(hours=9)
        return dt_kst.date()
    except: return None

# [ì‹ ê·œ] ì˜ìƒ ê¸¸ì´ íŒŒì‹± í•¨ìˆ˜ (PT1H2M10S -> ë¶„ ë‹¨ìœ„ ë³€í™˜)
def parse_duration_to_minutes(duration_str):
    if not duration_str: return 0.0
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(duration_str)
    if not match: return 0.0
    h, m, s = match.groups()
    total_sec = (int(h or 0) * 3600) + (int(m or 0) * 60) + (int(s or 0))
    return round(total_sec / 60, 1)
# endregion


# region [3. ì‹œê°í™” í•¨ìˆ˜ (Visualization)]
# ==========================================
# Plotlyë¥¼ ì´ìš©í•œ ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜ë“¤
# ==========================================
def get_pyramid_chart_and_df(stats_dict, total_views):
    if not stats_dict: return None, None, ""
    
    age_order = ['age13-17', 'age18-24', 'age25-34', 'age35-44', 'age45-54', 'age55-64', 'age65-']
    display_labels = [label.replace('age', '') for label in age_order]
    
    male_data = defaultdict(float); female_data = defaultdict(float)
    table_rows = []; total_male = 0; total_female = 0

    for key, count in stats_dict.items():
        parts = key.split('_')
        if len(parts) != 2: continue
        age_group, gender = parts[0], parts[1]
        if gender not in ['male', 'female']: continue
        if age_group not in age_order: continue 
        
        pct = (count / total_views) * 100 if total_views > 0 else 0
        clean_age = age_group.replace('age', '')
        
        if gender == 'male':
            male_data[clean_age] += pct; total_male += pct
        elif gender == 'female':
            female_data[clean_age] += pct; total_female += pct
            
        table_rows.append({
            "ì—°ë ¹": clean_age, "ì„±ë³„": "ë‚¨" if gender=='male' else "ì—¬", 
            "ì¡°íšŒìˆ˜": int(count), "ë¹„ìœ¨": pct
        })

    male_vals = [male_data[l] for l in display_labels]
    female_vals = [female_data[l] for l in display_labels]
    male_vals_neg = [-v for v in male_vals] 

    fig = go.Figure()
    fig.add_trace(go.Bar(y=display_labels, x=male_vals_neg, name='ë‚¨ì„±', orientation='h',
        marker=dict(color='#5684D5'), text=[f"{v:.1f}%" if v>0 else "" for v in male_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ë‚¨ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, male_vals)]))
    fig.add_trace(go.Bar(y=display_labels, x=female_vals, name='ì—¬ì„±', orientation='h',
        marker=dict(color='#FF7675'), text=[f"{v:.1f}%" if v>0 else "" for v in female_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ì—¬ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, female_vals)]))
    
    max_val = max(max(male_vals) if male_vals else 0, max(female_vals) if female_vals else 0)
    max_range = max_val * 1.2 if max_val > 0 else 10

    fig.update_layout(
        barmode='overlay',
        xaxis=dict(tickvals=[-max_range, 0, max_range], ticktext=[f"{max_range:.0f}%", "0%", f"{max_range:.0f}%"], range=[-max_range, max_range]),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=10, r=10, t=30, b=10),
        height=300,
        paper_bgcolor='rgba(0,0,0,0)', 
        plot_bgcolor='rgba(0,0,0,0)'
    )
    
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df['ì—°ë ¹'] = pd.Categorical(df['ì—°ë ¹'], categories=display_labels, ordered=True)
        df = df.sort_values(['ì—°ë ¹', 'ì„±ë³„'])

    title_str = f"ğŸ‘¥ ì„±ë³„/ì—°ë ¹ (ë‚¨ {total_male:.1f}% vs ì—¬ {total_female:.1f}%)"
    return fig, df, title_str

def get_traffic_chart(traffic_dict):
    if not traffic_dict: return None
    sorted_t = sorted(traffic_dict.items(), key=lambda x: x[1], reverse=True)
    labels = []; values = []
    for k, v in sorted_t[:5]:
        labels.append(map_traffic_source(k)); values.append(v)
    if len(sorted_t) > 5:
        labels.append("ê¸°íƒ€"); values.append(sum(v for k,v in sorted_t[5:]))
    
    if not values: return None
        
    teal_colors = ['#00b894', '#00cec9', '#55efc4', '#81ecec', '#b2bec3', '#dfe6e9']
    fig = px.pie(names=labels, values=values, hole=0.5, color_discrete_sequence=teal_colors)
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_keyword_bar_chart(keyword_dict):
    if not keyword_dict: return None
    
    sorted_k = sorted(keyword_dict.items(), key=lambda x: x[1], reverse=True)[:10]
    if not sorted_k: return None
    
    words = [k[0] for k in sorted_k][::-1] 
    counts = [k[1] for k in sorted_k][::-1]
    
    fig = go.Figure(go.Bar(
        x=counts, y=words, orientation='h',
        marker=dict(color='#fdcb6e'),
        text=[f"{int(v):,}" for v in counts], textposition='auto'
    ))
    
    fig.update_layout(
        margin=dict(l=10, r=10, t=10, b=10),
        height=300,
        xaxis=dict(showticklabels=False, visible=False),
        yaxis=dict(tickfont=dict(size=13)),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_channel_share_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)
    labels = []; values = []
    top_list = sorted_ch[:5]
    others_sum = sum(ch['total_views'] for ch in sorted_ch[5:])
    
    for ch in top_list:
        labels.append(ch['channel_name']); values.append(ch['total_views'])
    if len(sorted_ch) > 5:
        labels.append("ê·¸ ì™¸ ì±„ë„"); values.append(others_sum)
    
    if sum(values) == 0: return None
        
    colors = []
    if highlight_channel:
        for label in labels:
            colors.append('#6c5ce7' if label == highlight_channel else '#dfe6e9')
    else:
        colors = ['#6c5ce7', '#a29bfe', '#8e44ad', '#9b59b6', '#d6a2e8', '#dfe6e9']

    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=0.5, marker=dict(colors=colors))])
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_engagement_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)[:10]
    names = [c['channel_name'] for c in sorted_ch]
    like_ratios = [(c['total_likes']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    share_ratios = [(c['total_shares']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    
    if not names: return None

    base_like_color = '#ff7675'; base_share_color = '#74b9ff'
    if highlight_channel:
        like_colors = [base_like_color if n == highlight_channel else '#dfe6e9' for n in names]
        share_colors = [base_share_color if n == highlight_channel else '#dfe6e9' for n in names]
    else:
        like_colors = base_like_color; share_colors = base_share_color

    fig = go.Figure()
    fig.add_trace(go.Bar(x=names, y=like_ratios, name='ì¢‹ì•„ìš”(%)', marker_color=like_colors))
    fig.add_trace(go.Bar(x=names, y=share_ratios, name='ê³µìœ (%)', marker_color=share_colors))

    fig.update_layout(
        barmode='group',
        margin=dict(l=20, r=20, t=10, b=40),
        height=300,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis=dict(title="ë¹„ìœ¨(%)", tickformat=".2f"),
        xaxis=dict(tickangle=-45),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_country_map(country_stats):
    if not country_stats: return None
    data = []
    for k, v in country_stats.items():
        iso3 = ISO_MAPPING.get(k, k)
        c_name = k 
        if k == 'KR': c_name = 'ëŒ€í•œë¯¼êµ­'
        elif k == 'US': c_name = 'ë¯¸êµ­'
        elif k == 'JP': c_name = 'ì¼ë³¸'
        elif k == 'VN': c_name = 'ë² íŠ¸ë‚¨'
        elif k == 'TH': c_name = 'íƒœêµ­'
        data.append({'iso_alpha': iso3, 'views': v, 'country': c_name, 'fmt_views': format_korean_number(v)})
    
    if not data: return None
    df_map = pd.DataFrame(data)
    df_kr = df_map[df_map['iso_alpha'] == 'KOR']
    df_others = df_map[df_map['iso_alpha'] != 'KOR']
    
    fig = go.Figure()
    if not df_others.empty:
        fig.add_trace(go.Choropleth(
            locations=df_others['iso_alpha'], z=df_others['views'], text=df_others['country'],
            customdata=df_others[['country', 'fmt_views']], colorscale='Teal',
            marker_line_color='#ffffff', marker_line_width=0.5, showscale=True,
            colorbar=dict(title="ì¡°íšŒìˆ˜", x=1.0, len=0.8),
            hovertemplate="<b>%{customdata[0]}</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))
    if not df_kr.empty:
        fig.add_trace(go.Choropleth(
            locations=df_kr['iso_alpha'], z=[1]*len(df_kr), text=df_kr['country'],
            customdata=df_kr[['country', 'fmt_views']], colorscale=[[0, '#5684D5'], [1, '#5684D5']], 
            marker_line_color='#ffffff', marker_line_width=1, showscale=False,
            hovertemplate="<b>%{customdata[0]} (Home)</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))

    fig.update_geos(
        showcountries=True, countrycolor="#E0E0E0",
        showcoastlines=True, coastlinecolor="#E0E0E0",
        showframe=False, projection_type='natural earth',
        fitbounds="locations" if df_map.empty else False,
        bgcolor='rgba(0,0,0,0)'
    )
    fig.update_layout(
        margin=dict(l=0, r=0, t=0, b=0), 
        height=400, 
        dragmode='pan',
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_daily_trend_chart(daily_stats, recent_gap=0):
    """
    daily_stats: Analytics API ì¼ë³„ ì¡°íšŒìˆ˜
    recent_gap: Data API(ì‹¤ì‹œê°„) ì´í•© - Analytics ì´í•©
    """
    # [1] Analytics ë°ì´í„°(daily_stats)ê°€ ì—†ìœ¼ë©´, ë¦¬ì–¼íƒ€ì„ ë°ì´í„°ê°€ ìˆì–´ë„ ì°¨íŠ¸ ìƒì„± X
    if not daily_stats: return None
    
    dates = sorted(daily_stats.keys())
    views = [daily_stats[d] for d in dates]
    
    fig = go.Figure()
    
    # [2] í™•ì •ëœ Analytics ë°ì´í„° (ì‹¤ì„  - ë³´ë¼ìƒ‰)
    fig.add_trace(go.Scatter(
        x=dates, y=views, mode='lines+markers', name='í™•ì • ì¡°íšŒìˆ˜',
        line=dict(color='#6c5ce7', width=3), marker=dict(size=6)
    ))
    
    # [3] ì‹¤ì‹œê°„ ë°ì´í„° ì—°ê²° (ìˆì„ ê²½ìš°ë§Œ)
    if recent_gap > 0 and dates:
        last_date_str = dates[-1]
        last_val = views[-1]
        
        # ë‚ ì§œ ì¶©ëŒ ë°©ì§€: ë¦¬ì–¼íƒ€ì„ í¬ì¸íŠ¸ëŠ” ë¬´ì¡°ê±´ ë§ˆì§€ë§‰ Analytics ë‚ ì§œë³´ë‹¤ ë¯¸ë˜ì—¬ì•¼ í•¨
        # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ êµ¬í•˜ë˜, ë§ˆì§€ë§‰ Analytics ë‚ ì§œì™€ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ í•˜ë£¨ ë’¤ë¡œ ì„¤ì •
        today_dt = datetime.today()
        last_anl_dt = datetime.strptime(last_date_str, "%Y-%m-%d")
        
        if today_dt.date() <= last_anl_dt.date():
            target_dt = last_anl_dt + timedelta(days=1)
        else:
            target_dt = today_dt
            
        target_date_str = target_dt.strftime("%Y-%m-%d")
        
        # ì ì„  ê·¸ë˜í”„ ì¶”ê°€ (ë§ˆì§€ë§‰ í™•ì •ì¼ ~ íƒ€ê²Ÿì¼)
        fig.add_trace(go.Scatter(
            x=[last_date_str, target_date_str],
            y=[last_val, recent_gap],
            mode='lines+markers',
            name='ì‹¤ì‹œê°„(ì¶”ì •)',
            line=dict(color='#ff7675', width=3, dash='dot'), # ë¶‰ì€ ì ì„ 
            marker=dict(size=8, symbol='star')
        ))
        
        fig.add_annotation(
            x=target_date_str, y=recent_gap,
            text="Realtime (Est.)", showarrow=True, arrowhead=1,
            yshift=10, font=dict(color="#d63031", size=10)
        )

    # [4] Yì¶• í¬ë§· ì„¤ì • (ì½¤ë§ˆ)
    fig.update_layout(
        margin=dict(l=20, r=20, t=20, b=20),
        height=350, 
        xaxis=dict(title="ë‚ ì§œ", tickformat="%Y-%m-%d"),
        yaxis=dict(title="ì¡°íšŒìˆ˜", tickformat=","), # #,### í¬ë§·
        hovermode="x unified",
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_efficiency_scatter(video_details):
    if not video_details: return None
    df = pd.DataFrame(video_details)
    if df.empty: return None
    df = df[(df['duration_min'] > 0) & (df['avg_pct'].notnull()) & (df['avg_pct'] > 0)]
    if df.empty: return None
    
    fig = px.scatter(
        df, x='duration_min', y='avg_pct', 
        size='views', color='avg_pct',
        hover_name='title',
        labels={'duration_min': 'ì˜ìƒ ê¸¸ì´(ë¶„)', 'avg_pct': 'í‰ê·  ì§€ì†ë¥ (%)'},
        color_continuous_scale='Viridis'
    )
    fig.add_vline(x=df['duration_min'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    fig.add_hline(y=df['avg_pct'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    
    fig.update_layout(
        margin=dict(l=20, r=20, t=20, b=20),
        height=400,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=True, gridcolor='#eee'),
        yaxis=dict(showgrid=True, gridcolor='#eee')
    )
    return fig
# endregion


# region [4. API ë° ë°ì´í„° ì²˜ë¦¬ (API & Data Processing)]
# ==========================================
# Google API ì¸ì¦, ë°ì´í„° ë™ê¸°í™”, ë¶„ì„ ë¡œì§
# ==========================================
def get_creds_from_file(token_filename):
    creds = None
    if os.path.exists(token_filename):
        creds = google.oauth2.credentials.Credentials.from_authorized_user_file(token_filename, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(google.auth.transport.requests.Request())
                with open(token_filename, 'w') as token: token.write(creds.to_json())
            except: return None
        else: return None
    return creds

def process_sync_channel(token_file, limit_date, status_box, force_rescan):
    # ... [ì´ì „ ì½”ë“œì™€ ë™ì¼] ...
    file_label = os.path.basename(token_file).replace("token_", "").replace(".json", "")
    creds = get_creds_from_file(token_file)
    if not creds: 
        status_box.error(f"âŒ [{file_label}] í† í° ì˜¤ë¥˜")
        return None
    try:
        youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
        ch_res = youtube.channels().list(part='snippet,contentDetails', mine=True).execute()
        if not ch_res['items']: 
            status_box.warning(f"âš ï¸ [{file_label}] ì •ë³´ ì—†ìŒ")
            return None
        ch_info = ch_res['items'][0]; ch_name = ch_info['snippet']['title']
        uploads_id = ch_info['contentDetails']['relatedPlaylists']['uploads']
        cache_file = f"cache_nov_{token_file}"
        cached_videos = []
        
        cached_ids = set()
        
        if not force_rescan and os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f: cached_videos = json.load(f)
            cached_ids = {v['id'] for v in cached_videos}
            status_box.info(f"ğŸ”„ [{ch_name}] í™•ì¸ ì¤‘...")
        else: status_box.info(f"â³ [{ch_name}] ìŠ¤ìº” ì‹œì‘")
        
        new_videos = []; next_page_token = None; stop_scanning = False
        
        while not stop_scanning:
            req = youtube.playlistItems().list(part='snippet', playlistId=uploads_id, maxResults=50, pageToken=next_page_token)
            res = req.execute()
            for item in res['items']:
                vid = item['snippet']['resourceId']['videoId']
                title = item['snippet']['title']
                desc = item['snippet']['description']
                p_at = item['snippet']['publishedAt']
                if p_at < limit_date: stop_scanning = True; break
                
                if not force_rescan and vid in cached_ids: 
                    stop_scanning = True; break
                
                new_videos.append({'id': vid, 'title': title, 'date': p_at, 'description': desc})
            if len(new_videos) > 0 and len(new_videos) % 50 == 0:
                status_box.markdown(f"ğŸƒ **[{ch_name}]** +{len(new_videos)}")
            if not res.get('nextPageToken'): stop_scanning = True
            next_page_token = res.get('nextPageToken')
            if not next_page_token: stop_scanning = True
        final_list = new_videos + cached_videos if not force_rescan else new_videos
        if new_videos or force_rescan:
            with open(cache_file, 'w', encoding='utf-8') as f: json.dump(final_list, f, ensure_ascii=False, indent=2)
            status_box.success(f"âœ… **[{ch_name}]** ì™„ë£Œ (+{len(new_videos)})")
        else: status_box.success(f"âœ… **[{ch_name}]** ìµœì‹ ")
        return {'creds': creds, 'name': ch_name, 'videos': final_list}
    except Exception as e:
        status_box.error(f"âŒ ì—ëŸ¬: {str(e)}")
        return {'error': str(e)}

def process_analysis_channel(channel_data, keyword, vid_start, vid_end, anl_start, anl_end):
    creds = channel_data['creds']; videos = channel_data['videos']
    norm_keyword = normalize_text(keyword)
    target_ids = []
    id_map = {} 
    video_date_map = {} # ì˜ìƒë³„ ì—…ë¡œë“œ ë‚ ì§œ ì €ì¥ìš©
    
    # 1. ëŒ€ìƒ ì˜ìƒ í•„í„°ë§
    for v in videos:
        t_match = norm_keyword in normalize_text(v['title'])
        d_match = norm_keyword in normalize_text(v.get('description', ''))
        if not (t_match or d_match): continue
        
        v_dt_kst = parse_utc_to_kst_date(v['date']) # KST Date ê°ì²´
        if v_dt_kst and (vid_start <= v_dt_kst <= vid_end): 
            target_ids.append(v['id'])
            id_map[v['id']] = v['title']
            video_date_map[v['id']] = v['date'] # UTC ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ì €ì¥ ("2025-01-01T00:00:00Z")
            
    if not target_ids: return None
    
    yt_anl = googleapiclient.discovery.build('youtubeAnalytics', 'v2', credentials=creds)
    youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
    
    total_views = 0; total_likes = 0; total_shares = 0
    demo = defaultdict(float); traffic = defaultdict(float)
    country = defaultdict(float); daily = defaultdict(float)
    keywords_count = defaultdict(float)
    w_avg_sum = 0; v_for_avg = 0
    over_1m_count = 0 
    
    top_video_stats = []
    
    # 48ì‹œê°„ ê¸°ì¤€ì  ì„¤ì • (UTC ê¸°ì¤€)
    now_utc = datetime.utcnow()
    threshold_dt = now_utc - timedelta(hours=48)
    
    batch_size = 50 
    for i in range(0, len(target_ids), batch_size):
        batch_ids = target_ids[i : i + batch_size]
        vid_str = ",".join(batch_ids)
        
        # [A] Analytics ë°ì´í„° ìˆ˜ì§‘
        # ë°°ì¹˜ ì „ì²´ í•©ì‚° ë°ì´í„° (ê³µìœ , ì¸êµ¬í†µê³„ ë“± ê°œë³„ ë§¤í•‘ ì–´ë ¤ìš´ ê²ƒë“¤)
        anl_views_map = {} # ì˜ìƒë³„ Analytics ì¡°íšŒìˆ˜ ì €ì¥
        anl_likes_map = {} # ì˜ìƒë³„ Analytics ì¢‹ì•„ìš” ì €ì¥
        anl_retention_map = {} # ì˜ìƒë³„ ì§€ì†ë¥  ì €ì¥
        
        try:
            # 1. ì „ì²´ í•©ì‚°ìš© (ê³µìœ  ë“±)
            r_b = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='shares', filters=f'video=={vid_str}').execute()
            if 'rows' in r_b and r_b['rows']:
                total_shares += r_b['rows'][0][0] # ê³µìœ ëŠ” Data APIì— ì—†ìœ¼ë¯€ë¡œ Analytics ì „ì ìœ¼ë¡œ ì‹ ë¢°

            # 2. ì˜ìƒë³„ ìƒì„¸ ë°ì´í„° (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ì§€ì†ë¥ ) -> 48ì‹œê°„ ë¡œì§ ì ìš©ì„ ìœ„í•´ 'dimensions=video'ë¡œ ìª¼ê°œì„œ ë°›ìŒ
            r_v = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views,likes,averageViewPercentage', dimensions='video', filters=f'video=={vid_str}').execute()
            if 'rows' in r_v and r_v['rows']:
                for r in r_v['rows']:
                    # r[0]: video_id, r[1]: views, r[2]: likes, r[3]: avg_pct
                    anl_views_map[r[0]] = r[1]
                    anl_likes_map[r[0]] = r[2]
                    anl_retention_map[r[0]] = r[3]

            # 3. ê¸°íƒ€ ì°¨íŠ¸ìš© ë°ì´í„° (ì´ê±´ í•©ì‚°ì¹˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            # ì¸êµ¬í†µê³„
            r_d = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='viewerPercentage', dimensions='ageGroup,gender', filters=f'video=={vid_str}').execute()
            # ë°°ì¹˜ ì „ì²´ ë·°ìˆ˜ êµ¬í•˜ê¸° (ê°€ì¤‘ì¹˜ ê³„ì‚°ìš©)
            batch_total_view_anl = sum(anl_views_map.values())
            if 'rows' in r_d and r_d['rows']:
                for r in r_d['rows']: demo[f"{r[0]}_{r[1]}"] += batch_total_view_anl * (r[2] / 100)

            # ìœ ì…ê²½ë¡œ
            r_t = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceType', filters=f'video=={vid_str}').execute()
            if 'rows' in r_t and r_t['rows']:
                for r in r_t['rows']: traffic[r[0]] += r[1]
            
            # ê²€ìƒ‰ì–´ (Top 15)
            try:
                r_k = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceDetail', filters=f'video=={vid_str};insightTrafficSourceType==YT_SEARCH', maxResults=15, sort='-views').execute()
                if 'rows' in r_k and r_k['rows']:
                    for r in r_k['rows']:
                        if r[0] != 'GOOGLE_SEARCH': keywords_count[r[0]] += r[1]
            except: pass

            # êµ­ê°€
            r_c = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='country', filters=f'video=={vid_str}', maxResults=50).execute()
            if 'rows' in r_c and r_c['rows']:
                for r in r_c['rows']: country[r[0]] += r[1]

            # ì¼ë³„ ì¶”ì´
            r_day = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='day', filters=f'video=={vid_str}', sort='day').execute()
            if 'rows' in r_day and r_day['rows']:
                for r in r_day['rows']: daily[r[0]] += r[1]
        
        except: pass

        # [B] Data API (ì‹¤ì‹œê°„) ë°ì´í„° ìˆ˜ì§‘ ë° [C] í•˜ì´ë¸Œë¦¬ë“œ í•©ì‚°
        try:
            rt_res = youtube.videos().list(part='statistics,contentDetails', id=vid_str).execute()
            
            rt_stats_map = {}
            rt_content_map = {}
            if 'items' in rt_res:
                for item in rt_res['items']:
                    rt_stats_map[item['id']] = item['statistics']
                    rt_content_map[item['id']] = item['contentDetails']

            # === [í•µì‹¬ ë¡œì§] ì˜ìƒë³„ ë‚ ì§œ ë¹„êµí•˜ì—¬ í•©ì‚° ===
            for vid_id in batch_ids:
                # 1. ì˜ìƒ ë‚ ì§œ í™•ì¸
                v_date_str = video_date_map.get(vid_id)
                if not v_date_str: continue
                
                v_upload_dt = datetime.strptime(v_date_str, "%Y-%m-%dT%H:%M:%SZ")
                is_recent = v_upload_dt > threshold_dt # 48ì‹œê°„ ì´ë‚´ ì—…ë¡œë“œ ì—¬ë¶€

                # 2. ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
                # Analytics ë°ì´í„°
                a_v = anl_views_map.get(vid_id, 0)
                a_l = anl_likes_map.get(vid_id, 0)
                a_pct = anl_retention_map.get(vid_id, 0)
                
                # Data API ë°ì´í„° (ì‹¤ì‹œê°„)
                stats = rt_stats_map.get(vid_id, {})
                rt_v = int(stats.get('viewCount', 0))
                rt_l = int(stats.get('likeCount', 0))
                
                # 3. ê²°ì • ë¡œì§ (User Request)
                final_v = 0
                final_l = 0
                
                if is_recent:
                    # [Case 1] ìµœì‹  ì˜ìƒ (<48h) -> Data API (ì‹¤ì‹œê°„ ëˆ„ì ê°’) ì±„íƒ
                    final_v = rt_v
                    final_l = rt_l
                else:
                    # [Case 2] ì˜¤ë˜ëœ ì˜ìƒ (>48h) -> Analytics API (ê¸°ê°„ ë‚´ ë°ì´í„°) ì±„íƒ
                    final_v = a_v
                    final_l = a_l
                
                # 4. ì´ê³„ í•©ì‚°
                total_views += final_v
                total_likes += final_l
                
                # ê°€ì¤‘ í‰ê·  (ì§€ì†ë¥ ì€ Analyticsì—ë§Œ ìˆìŒ. ìµœì‹  ì˜ìƒì€ ì§€ì†ë¥  0ì¼ í™•ë¥  ë†’ìŒ)
                if final_v > 0 and a_pct > 0:
                    w_avg_sum += (final_v * a_pct)
                    v_for_avg += final_v
                
                # 100ë§Œ ì¹´ìš´íŠ¸ (ì´ê±´ í•­ìƒ ì‹¤ì‹œê°„ ê¸°ì¤€ì´ ì •í™•í•¨ - ëª…ì˜ˆì˜ ì „ë‹¹ ëŠë‚Œ)
                if rt_v >= 1000000: over_1m_count += 1

                # 5. Top ë¦¬ìŠ¤íŠ¸ìš© ë°ì´í„° (ë¦¬ìŠ¤íŠ¸ì—ëŠ” í•­ìƒ 'í˜„ì¬ ìƒíƒœ'ë¥¼ ë³´ì—¬ì£¼ëŠ”ê²Œ ì¢‹ìŒ -> Data API ì‚¬ìš©)
                # ë‹¨, 'ê¸°ê°„ ë‚´ ì¡°íšŒìˆ˜'ë¥¼ ë³´ì—¬ì£¼ê³  ì‹¶ë‹¤ë©´ final_vë¥¼ ì¨ì•¼ í•¨.
                # ë³´í†µ ë¦¬ìŠ¤íŠ¸ëŠ” "ì´ ì˜ìƒì˜ í˜„ì¬ ìŠ¤í™"ì„ ë³´ëŠ” ìš©ë„ì´ë¯€ë¡œ rt_v(ì‹¤ì‹œê°„) ìœ ì§€
                if rt_v > 0:
                    top_video_stats.append({
                        'id': vid_id,
                        'title': id_map.get(vid_id, 'Unknown'),
                        'views': rt_v,       # ë¦¬ìŠ¤íŠ¸ í‘œì‹œìš©: ì‹¤ì‹œê°„ ì¡°íšŒìˆ˜
                        'likes': rt_l,       # ë¦¬ìŠ¤íŠ¸ í‘œì‹œìš©: ì‹¤ì‹œê°„ ì¢‹ì•„ìš”
                        'period_views': final_v, # (ì˜µì…˜) ê¸°ê°„ ë‚´ ì¡°íšŒìˆ˜
                        'avg_pct': a_pct if a_pct > 0 else None,
                        'duration_min': parse_duration_to_minutes(rt_content_map.get(vid_id, {}).get('duration'))
                    })

        except Exception as e:
            print(f"Error processing batch: {e}")
            pass

        time.sleep(0.05)

    if not top_video_stats and total_views == 0: return None
    
    top_video_stats.sort(key=lambda x: x['views'], reverse=True)

    return {
        'channel_name': channel_data['name'], 'video_count': len(target_ids),
        'total_views': total_views, 'total_likes': total_likes, 'total_shares': total_shares,
        'avg_view_pct': (w_avg_sum/v_for_avg) if v_for_avg > 0 else 0,
        'demo_counts': demo, 'traffic_counts': traffic,
        'country_counts': country, 'daily_stats': daily,
        'keywords_counts': keywords_count,
        'top_video_stats': top_video_stats,
        'over_1m_count': over_1m_count 
    }
# endregion


# region [5. ë©”ì¸ UI ë° ì‹¤í–‰ ë¡œì§ (Main UI & Execution)]
# ==========================================
# ì‚¬ì´ë“œë°”, ë©”ì¸ ëŒ€ì‹œë³´ë“œ UI, ì‹¤í–‰ ì»¨íŠ¸ë¡¤
# ==========================================
st.title("ğŸ“Š Drama YouTube Insight")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ›ï¸ ë°ì´í„° ê´€ë¦¬ ì„¼í„°")
    token_files = glob.glob("token_*.json")
    st.markdown("---")
    st.caption("ë°ì´í„° ë™ê¸°í™”")
    if st.button("ğŸ”„ ìµœì‹  ì˜ìƒ ì—…ë°ì´íŠ¸", type="primary", use_container_width=True):
        if not token_files: st.error("ì—°ë™ëœ í† í° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.session_state['channels_data'] = []
            st.write("--- ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘ ---")
            placeholders = {tf: st.empty() for tf in token_files}
            ready = []
            ctx = get_script_run_ctx()
            def sync_worker(tf, sb):
                add_script_run_ctx(ctx=ctx)
                return process_sync_channel(tf, DEFAULT_LIMIT_DATE, sb, False)
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                futs = {ex.submit(sync_worker, tf, placeholders[tf]): tf for tf in token_files}
                for f in as_completed(futs):
                    res = f.result()
                    if res and 'name' in res: ready.append(res)
            st.session_state['channels_data'] = ready
            if ready: st.success("ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            
    st.markdown("---")
    with st.expander("ğŸ”’ ê³ ê¸‰ ê´€ë¦¬ì ì„¤ì •"):
        if 'admin_unlocked' not in st.session_state: st.session_state['admin_unlocked'] = False
        if not st.session_state['admin_unlocked']:
            if st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="pw_input") == "dima1234":
                st.session_state['admin_unlocked'] = True
                st.rerun()
        if st.session_state['admin_unlocked']:
            st.success("âœ… ê´€ë¦¬ì ëª¨ë“œ On")
            l_date = st.date_input("ìˆ˜ì§‘ ë§ˆì§€ë…¸ì„ ", value=pd.to_datetime(DEFAULT_LIMIT_DATE))
            if st.button("ğŸš¨ ì „ì²´ ì¬ìˆ˜ì§‘", type="secondary"):
                st.session_state['channels_data'] = []
                placeholders = {tf: st.empty() for tf in token_files}
                ready = []
                ctx = get_script_run_ctx()
                def deep_worker(tf, sb):
                    add_script_run_ctx(ctx=ctx)
                    return process_sync_channel(tf, l_date.strftime("%Y-%m-%d"), sb, True)
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                    futs = {ex.submit(deep_worker, tf, placeholders[tf]): tf for tf in token_files}
                    for f in as_completed(futs):
                        res = f.result()
                        if res and 'name' in res: ready.append(res)
                st.session_state['channels_data'] = ready
                if ready: st.success("ì™„ë£Œ!")
            if st.button("ğŸ”’ ì ê¸ˆ"):
                st.session_state['admin_unlocked'] = False
                st.rerun()

# --- ë©”ì¸ ---
if 'channels_data' not in st.session_state or not st.session_state['channels_data']:
    token_files = glob.glob("token_*.json")
    temp_data = []
    for tf in token_files:
        cf = f"cache_nov_{tf}"
        if os.path.exists(cf):
             with open(cf, 'r', encoding='utf-8') as f:
                try:
                    vids = json.load(f); creds = get_creds_from_file(tf)
                    if creds:
                        lbl = os.path.basename(tf).replace("token_", "").replace(".json", "")
                        temp_data.append({'creds': creds, 'name': lbl, 'videos': vids})
                except: pass
    if temp_data: st.session_state['channels_data'] = temp_data
    else: st.info("ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ì‚¬ì´ë“œë°”ì—ì„œ [ìµœì‹  ì˜ìƒ ì—…ë°ì´íŠ¸]ë¥¼ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”.")

if 'channels_data' in st.session_state and st.session_state['channels_data']:
    data = st.session_state['channels_data']
    tv = sum(len(c['videos']) for c in data)
    
    st.markdown(f"""
    <div style='background-color:white; padding:10px 20px; border-radius:8px; border:1px solid #eee; margin-bottom:20px; display:flex; align-items:center; gap:10px;'>
        <span>âœ… <b>ì—°ë™ ìƒíƒœ:</b> ì±„ë„ <span style='color:#2980b9; font-weight:bold'>{len(data)}ê°œ</span></span>
        <span style='color:#ddd'>|</span>
        <span>ğŸ“ <b>DB ì˜ìƒ:</b> <span style='color:#2980b9; font-weight:bold'>{tv:,}ê°œ</span></span>
    </div>
    """, unsafe_allow_html=True)

    today = datetime.today()
    first_day = today.replace(day=1)

    with st.form("analysis_form"):
        st.subheader("ğŸ” í†µí•© ë¶„ì„ ì„¤ì •")
        c1, c2, c3 = st.columns([2, 1, 1])
        with c1: keyword = st.text_input("ë¶„ì„ IP", placeholder="ì˜ˆ: ëˆˆë¬¼ì˜ ì—¬ì™•")
        with c2: v_dates = st.date_input("ì˜ìƒ ì—…ë¡œë“œ ê¸°ê°„", value=(first_day, today))
        with c3: a_dates = st.date_input("ë°ì´í„° ì‚°ì¶œ ê¸°ê°„", value=(first_day, today))
        submit_btn = st.form_submit_button("ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

    if submit_btn:
        if isinstance(v_dates, tuple):
            v_start = v_dates[0]; v_end = v_dates[1] if len(v_dates)>1 else v_dates[0]
        else: v_start = v_end = v_dates
        if isinstance(a_dates, tuple):
            a_start = a_dates[0]; a_end = a_dates[1] if len(a_dates)>1 else a_dates[0]
        else: a_start = a_end = a_dates

        if not keyword.strip(): st.error("âš ï¸ ë¶„ì„ IPë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            vs_str, ve_str = v_start, v_end
            as_str = a_start.strftime("%Y-%m-%d"); ae_str = a_end.strftime("%Y-%m-%d")
            
            prog_bar = st.progress(0, text="ë°ì´í„° ë¶„ì„ ì¤‘...")
            
            # ì—¬ê¸°ì„œ ê³„ì‚°ì€ ì¼ë‹¨ ì±„ë„ë³„ë¡œ ë‹¤ ìˆ˜í–‰í•´ì„œ listì— ë‹´ìŒ
            ch_details_results = []
            
            ctx = get_script_run_ctx()
            def worker(cd, kw, vs, ve, ast, aet):
                add_script_run_ctx(ctx=ctx)
                return process_analysis_channel(cd, kw, vs, ve, ast, aet)

            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                futures = {ex.submit(worker, ch, keyword, vs_str, ve_str, as_str, ae_str): ch for ch in data}
                done = 0
                for f in as_completed(futures):
                    done += 1
                    prog_bar.progress(done/len(data), text=f"ì±„ë„ ë¶„ì„ ì¤‘... ({done}/{len(data)})")
                    res = f.result()
                    if res: ch_details_results.append(res)

            prog_bar.empty()
            
            # ì›ë³¸ ë°ì´í„° ì„¸ì…˜ ì €ì¥
            st.session_state['analysis_raw_results'] = ch_details_results
            st.session_state['analysis_keyword'] = keyword

    # --- ê²°ê³¼ ë Œë”ë§ ì„¸ì…˜ (ì—¬ê¸°ì„œ ì§‘ê³„ ë° ì„ íƒ í•„í„°ë§ ìˆ˜í–‰) ---
    if 'analysis_raw_results' in st.session_state and st.session_state['analysis_raw_results']:
        raw_data = st.session_state['analysis_raw_results']
        current_kw = st.session_state['analysis_keyword']
        
        st.divider()
        st.markdown(f"### ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸: <span style='color:#2980b9;'>{current_kw}</span>", unsafe_allow_html=True)
        
        # [2. ìˆ˜ì •] ì±„ë„ ì„ íƒê¸° (ì‘ê²Œ ìƒë‹¨ ë°°ì¹˜)
        ch_names = sorted([d['channel_name'] for d in raw_data])
        sel_options = ["ì „ì²´ ì±„ë„ í•©ì‚°"] + ch_names
        
        c_sel_col, _ = st.columns([1, 2])
        with c_sel_col:
            selected_ch = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì±„ë„ ì„ íƒ", sel_options, label_visibility="collapsed")
        
        # --- ì„ íƒì— ë”°ë¥¸ ì‹¤ì‹œê°„ ì§‘ê³„ (Aggregation) ---
        if selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°":
            target_data = raw_data
        else:
            target_data = [d for d in raw_data if d['channel_name'] == selected_ch]
            
        # ì§‘ê³„ ë³€ìˆ˜ ì´ˆê¸°í™”
        final_views = 0; final_likes = 0; final_shares = 0
        final_over_1m = 0; final_vid_count = 0
        final_stats = defaultdict(float); final_traffic = defaultdict(float)
        final_country = defaultdict(float); final_daily = defaultdict(float)
        final_keywords = defaultdict(float)
        w_avg_sum = 0; v_for_avg = 0
        final_top_videos = []
        
        for d in target_data:
            final_views += d['total_views']
            final_likes += d['total_likes']
            final_shares += d['total_shares']
            final_over_1m += d['over_1m_count']
            final_vid_count += d['video_count']
            
            # í‰ê·  ì§€ì†ë¥  ê°€ì¤‘ì¹˜ìš©
            if d['avg_view_pct'] > 0 and d['total_views'] > 0:
                w_avg_sum += (d['avg_view_pct'] * d['total_views'])
                v_for_avg += d['total_views']
            
            for k, v in d['demo_counts'].items(): final_stats[k] += v
            for k, v in d['traffic_counts'].items(): final_traffic[k] += v
            for k, v in d['country_counts'].items(): final_country[k] += v
            for k, v in d['daily_stats'].items(): final_daily[k] += v
            for k, v in d['keywords_counts'].items(): final_keywords[k] += v
            if 'top_video_stats' in d: final_top_videos.extend(d['top_video_stats'])
            
        final_avg_pct = (w_avg_sum / v_for_avg) if v_for_avg > 0 else 0
        
        # [3. ìˆ˜ì •] ì¼ë³„ ì¶”ì´ìš© Gap ê³„ì‚° (Hybrid Total - Analytics Sum)
        anl_total_daily = sum(final_daily.values())
        recent_gap = final_views - anl_total_daily
        
        # --- UI ê·¸ë¦¬ê¸° ---
        safe_anl_date = datetime.now() - timedelta(days=3)
        safe_str = safe_anl_date.strftime("%Y-%m-%d")
        
        if final_views > 0 or len(final_top_videos) > 0:
            st.caption(f"â„¹ï¸ **ë°ì´í„° ê¸°ì¤€**: ì¸êµ¬í†µê³„/ê²½ë¡œ ë“±ì€ **~{safe_str}** í™•ì •ì¹˜, **ì´ ì¡°íšŒìˆ˜/ì¢‹ì•„ìš” ë° ë¦¬ìŠ¤íŠ¸**ëŠ” **ì‹¤ì‹œê°„(Realtime)** ë°ì´í„°ì…ë‹ˆë‹¤.")

            # [ì„¹ì…˜ 0] í•µì‹¬ ì§€í‘œ
            m1, m2, m3, m4, m5, m6 = st.columns(6)
            m1.metric("ì´ ì¡°íšŒìˆ˜", f"{int(final_views):,}")
            m2.metric("ë¶„ì„ ì˜ìƒ", f"{final_vid_count:,}ê°œ")
            m3.metric("100ë§Œ+ ì˜ìƒ", f"{final_over_1m:,}ê°œ")
            m4.metric("í‰ê·  ì‹œì²­ì§€ì†ë¥ ", f"{final_avg_pct:.1f}%")
            m5.metric("ì´ ì¢‹ì•„ìš”", f"{int(final_likes):,}")
            m6.metric("ì´ ê³µìœ ", f"{int(final_shares):,}")
            st.write("")

            # [ì„¹ì…˜ 1] ì„±ë³„/ì—°ë ¹ (ìˆì„ ë•Œë§Œ)
            fig_demo, df_table, _ = get_pyramid_chart_and_df(final_stats, final_views)
            if fig_demo:
                c1, c2 = st.columns([1.6, 1])
                with c1:
                    st.markdown("##### ğŸ‘¥ ì„±ë³„/ì—°ë ¹ ë¶„í¬")
                    with st.container(border=True):
                        st.plotly_chart(fig_demo, use_container_width=True)
                with c2:
                    st.markdown("##### ğŸ“‹ ìƒì„¸ ë°ì´í„°")
                    with st.container(border=True):
                        if not df_table.empty:
                            df_disp = df_table.copy()
                            df_disp['ì¡°íšŒìˆ˜'] = df_disp['ì¡°íšŒìˆ˜'].apply(lambda x: f"{x:,}")
                            df_disp['ë¹„ìœ¨'] = df_disp['ë¹„ìœ¨'].apply(lambda x: f"{x:.1f}%")
                            st.dataframe(df_disp, use_container_width=True, hide_index=True, height=300)
                st.write("")

            # [1. ìˆ˜ì •] ì¼ë³„ ì¡°íšŒìˆ˜ ì¶”ì´ (ì „ì²´ 1í–‰)
            # Analytics ë°ì´í„°ê°€ ì—†ì–´ë„ Gap(Data API)ì´ ìˆìœ¼ë©´ ì°¨íŠ¸ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•´ ì¡°ê±´ ì™„í™”
            fig_trend = get_daily_trend_chart(final_daily, recent_gap)
            if fig_trend:
                st.markdown("##### ğŸ“ˆ ì¼ë³„ ì¡°íšŒìˆ˜ ì¶”ì´ (Analytics + Realtime Gap)")
                with st.container(border=True):
                    st.plotly_chart(fig_trend, use_container_width=True)
                st.write("")

            # [1. ìˆ˜ì •] ì¸ê¸° ì˜ìƒ ë¦¬ìŠ¤íŠ¸ (ì „ì²´ 1í–‰)
            st.markdown("##### ğŸ¥‡ ì¸ê¸° ì˜ìƒ TOP 100 (ì‹¤ì‹œê°„ ê¸°ì¤€)")
            with st.container(border=True):
                if final_top_videos:
                    unique_vids_map = {v['id']: v for v in final_top_videos}
                    deduped_vids = list(unique_vids_map.values())
                    top_vids = sorted(deduped_vids, key=lambda x: x['views'], reverse=True)[:100]
                    
                    df_top = pd.DataFrame(top_vids)
                    df_top['link'] = df_top['id'].apply(lambda x: f"https://youtu.be/{x}")
                    df_show = df_top[['title', 'views', 'avg_pct', 'likes', 'link']].copy()
                    df_show.columns = ['ì œëª©', 'ì¡°íšŒìˆ˜', 'ì§€ì†ë¥ (%)', 'ì¢‹ì•„ìš”', 'ë°”ë¡œê°€ê¸°']
                    df_show['ì¡°íšŒìˆ˜'] = df_show['ì¡°íšŒìˆ˜'].apply(lambda x: f"{int(x):,}")
                    df_show['ì¢‹ì•„ìš”'] = df_show['ì¢‹ì•„ìš”'].apply(lambda x: f"{int(x):,}")
                    
                    st.data_editor(
                        df_show,
                        column_config={
                            "ë°”ë¡œê°€ê¸°": st.column_config.LinkColumn(display_text="Watch ğŸ¬"),
                            "ì§€ì†ë¥ (%)": st.column_config.NumberColumn(format="%.1f%%"),
                        },
                        hide_index=True, use_container_width=True, disabled=True
                    )
                else: st.caption("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.write("")

            # [ì„¹ì…˜ 2] ìœ ì…/ê²€ìƒ‰ì–´
            fig_traffic = get_traffic_chart(final_traffic)
            fig_keywords = get_keyword_bar_chart(final_keywords)
            
            # [4. ìˆ˜ì •] ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆì–´ì•¼ í–‰ ìƒì„±
            if fig_traffic or fig_keywords:
                r2_1, r2_2 = st.columns(2)
                with r2_1:
                    if fig_traffic:
                        st.markdown("##### ğŸš¦ ìœ ì… ê²½ë¡œ Top 5")
                        with st.container(border=True):
                            st.plotly_chart(fig_traffic, use_container_width=True)
                with r2_2:
                    if fig_keywords:
                        st.markdown("##### ğŸ” Top 10 ê²€ìƒ‰ì–´ (SEO)")
                        with st.container(border=True):
                            st.plotly_chart(fig_keywords, use_container_width=True)
                st.write("")
            
            # [ì„¹ì…˜ 3] ì ìœ ìœ¨/ë°˜ì‘
            # ì ìœ ìœ¨ì€ 'ì „ì²´' ë³´ê¸°ì¼ ë•Œë§Œ ì˜ë¯¸ê°€ ìˆìœ¼ë¯€ë¡œ, 'ì „ì²´ ì±„ë„ í•©ì‚°'ì¼ ë•Œë§Œ í‘œì‹œí•˜ê±°ë‚˜,
            # íŠ¹ì • ì±„ë„ ì„ íƒ ì‹œ ì „ì²´ ëŒ€ë¹„ ì ìœ ìœ¨ì„ ë³´ì—¬ì£¼ëŠ” ë¡œì§ í•„ìš”.
            # ì—¬ê¸°ì„œëŠ” í¸ì˜ìƒ ì ìœ ìœ¨ ì°¨íŠ¸ëŠ” ì „ì²´ ë³´ê¸° ëª¨ë“œì—ì„œë§Œ, ë°˜ì‘ ì°¨íŠ¸ëŠ” í•­ìƒ í‘œì‹œ
            
            show_share = (selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°") and (len(raw_data) > 1)
            fig_share = get_channel_share_chart(raw_data, highlight_channel=None) if show_share else None
            # íŠ¹ì • ì±„ë„ ì„ íƒì‹œì—” ê·¸ ì±„ë„ì„ í•˜ì´ë¼ì´íŠ¸í•´ì„œ ë³´ì—¬ì¤„ ìˆ˜ë„ ìˆìŒ
            if selected_ch != "ì „ì²´ ì±„ë„ í•©ì‚°" and len(raw_data) > 1:
                fig_share = get_channel_share_chart(raw_data, highlight_channel=selected_ch)
            
            fig_engage = get_engagement_chart(target_data, highlight_channel=selected_ch if selected_ch!="ì „ì²´ ì±„ë„ í•©ì‚°" else None)

            if fig_share or fig_engage:
                r3_1, r3_2 = st.columns(2)
                with r3_1:
                    if fig_share:
                        st.markdown("##### ğŸ† ì±„ë„ë³„ ì ìœ ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_share, use_container_width=True)
                with r3_2:
                    if fig_engage:
                        st.markdown("##### â¤ï¸ ì¢‹ì•„ìš”/ê³µìœ  ë¹„ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_engage, use_container_width=True)
                st.write("")

            # [ì„¹ì…˜ 4] ì§€ë„
            fig_map = get_country_map(final_country)
            if fig_map:
                st.markdown("##### ğŸŒ ê¸€ë¡œë²Œ ì¡°íšŒìˆ˜ ë¶„í¬")
                with st.container(border=True):
                    st.plotly_chart(fig_map, use_container_width=True)
                st.write("")

            # [ì„¹ì…˜ 5] íš¨ìœ¨ì„± (Scatter)
            if final_top_videos:
                valid_scatter_vids = [v for v in final_top_videos if v.get('avg_pct') is not None and v.get('avg_pct') > 0]
                fig_scatter = get_efficiency_scatter(valid_scatter_vids)
                if fig_scatter:
                    st.markdown("##### âš¡ ì˜ìƒ íš¨ìœ¨ì„± ë§¤íŠ¸ë¦­ìŠ¤ (ê¸¸ì´ vs ì§€ì†ë¥ )")
                    st.caption("ìš°ìƒë‹¨ì— ìœ„ì¹˜í• ìˆ˜ë¡ ì˜ìƒë„ ê¸¸ê³  ëê¹Œì§€ ë³´ëŠ” ê³ íš¨ìœ¨ ì½˜í…ì¸ ì…ë‹ˆë‹¤.")
                    with st.container(border=True):
                        st.plotly_chart(fig_scatter, use_container_width=True)

        else:
            st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
# endregion
