import streamlit as st
import os
import glob
import json
import time
import re
import hashlib
import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import google.oauth2.credentials
import googleapiclient.discovery
import google.auth.transport.requests
import extra_streamlit_components as stx 
import google.generativeai as genai
from googleapiclient.errors import HttpError
import html
from pathlib import Path

from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
from datetime import datetime, timedelta
from github import Github, GithubException # PyGithub

# region [1. ì„¤ì • ë° ìƒìˆ˜ (Config & Constants)]
# ==========================================
st.set_page_config(
    page_title="Drama YouTube Insight", 
    page_icon="ğŸ“Š",
    layout="wide", 
    initial_sidebar_state="expanded"
)
# endregion


# region [1-1. ì…ì¥ê²Œì´íŠ¸ (ë³´ì•ˆ ì¸ì¦)]
# ==========================================
# Dashboard.pyì—ì„œ ì´ì‹ëœ ì¿ í‚¤/ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ë¡œì§
# ==========================================

def _rerun():
    """ìŠ¤íŠ¸ë¦¼ë¦¿ ë²„ì „ í˜¸í™˜ ë¦¬ëŸ° í•¨ìˆ˜"""
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()

def get_cookie_manager():
    # ì¿ í‚¤ ë§¤ë‹ˆì €ëŠ” í‚¤(Key)ê°€ ê³ ìœ í•´ì•¼ í•¨
    return stx.CookieManager(key="yt_auth_cookie_manager")

def _hash_password(password: str) -> str:
    return hashlib.sha256(str(password).encode()).hexdigest()

def check_password_with_cookie() -> bool:
    """
    1. Secrets ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    2. ì¿ í‚¤(ê³¼ê±° ë¡œê·¸ì¸) í™•ì¸
    3. ì„¸ì…˜(í˜„ì¬ ë¡œê·¸ì¸) í™•ì¸
    4. ì‹¤íŒ¨ ì‹œ ë¡œê·¸ì¸ì°½ ë„ìš°ê³  False ë°˜í™˜ -> ì•± ì¤‘ë‹¨
    """
    cookie_manager = get_cookie_manager()
    
    # Secretsì—ì„œ ë¹„ë°€ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì—ëŸ¬)
    # secrets.toml íŒŒì¼ì— [general] ì„¹ì…˜ í˜¹ì€ ìµœìƒë‹¨ì— DASHBOARD_PASSWORD = "..." ê°€ ìˆì–´ì•¼ í•¨
    secret_pwd = st.secrets.get("DASHBOARD_PASSWORD")
    if not secret_pwd:
        # í˜¸í™˜ì„±ì„ ìœ„í•´ general ì„¹ì…˜ë„ ì²´í¬
        if "general" in st.secrets:
            secret_pwd = st.secrets["general"].get("DASHBOARD_PASSWORD")
            
    if not secret_pwd:
        st.error("ğŸ”’ ì„¤ì • ì˜¤ë¥˜: Secretsì— 'DASHBOARD_PASSWORD'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
        
    hashed_secret = _hash_password(str(secret_pwd))
    
    # ì¿ í‚¤ ì½ê¸°
    cookies = cookie_manager.get_all()
    COOKIE_NAME = "yt_dashboard_auth"
    current_token = cookies.get(COOKIE_NAME)
    
    # ì¸ì¦ ê²€ì‚¬
    is_cookie_valid = (current_token == hashed_secret)
    is_session_valid = st.session_state.get("auth_success", False)
    
    if is_cookie_valid or is_session_valid:
        if is_cookie_valid and not is_session_valid:
            st.session_state["auth_success"] = True
        return True

    # ë¡œê·¸ì¸ UI
    st.markdown("#### ğŸ”’ Access Restricted")
    st.caption("ê´€ê³„ì ì™¸ ì ‘ê·¼ì´ ì œí•œëœ í˜ì´ì§€ì…ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        input_pwd = st.text_input("Password", type="password", key="login_pw_input")
        login_btn = st.button("Login", type="primary", use_container_width=True)

    if login_btn:
        if _hash_password(input_pwd) == hashed_secret:
            # ì¿ í‚¤ êµ½ê¸° (1ì¼ ìœ íš¨)
            expires = datetime.now() + timedelta(days=1)
            cookie_manager.set(COOKIE_NAME, hashed_secret, expires_at=expires)
            
            st.session_state["auth_success"] = True
            st.success("âœ… ì¸ì¦ ì„±ê³µ")
            time.sleep(0.5)
            _rerun()
        else:
            st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
    return False

# ğŸ›‘ [ì¤‘ìš”] ì—¬ê¸°ì„œ ì¸ì¦ ì‹¤íŒ¨ ì‹œ ì•± ì‹¤í–‰ì„ ë©ˆì¶¤
if not check_password_with_cookie():
    st.stop()

# ==========================================
# ì¸ì¦ í†µê³¼ í›„ ì‹¤í–‰ë˜ëŠ” ì˜ì—­
# ==========================================
# endregion


# region [1-2. ë°°í¬ í™˜ê²½ ì„¤ì • (Secrets ë³µì›)]
# ==========================================
# Streamlit Secretsì— ì €ì¥ëœ í† í° ì •ë³´ë¥¼ ì½ì–´ ë¡œì»¬ íŒŒì¼ë¡œ ë³µì›
if "tokens" in st.secrets:
    for file_name, content in st.secrets["tokens"].items():
        if not file_name.endswith(".json"):
            file_name += ".json"
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(file_name):
            with open(file_name, "w", encoding='utf-8') as f:
                f.write(content)
# endregion


# region [1-3. ë””ìì¸ ë° ìƒìˆ˜]
# ==========================================
# UI ë””ìì¸ CSS
custom_css = """
    <style>
        /* í—¤ë” íˆ¬ëª…í™” */
        header[data-testid="stHeader"] { background: transparent; }
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        [data-testid="stDecoration"] {display: none;}
        
        .block-container { padding-top: 1rem; padding-bottom: 3rem; }
        .stApp { background-color: #f8f9fa; }

        /* ì¹´ë“œ ë° ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ */
        div[data-testid="stMetric"] {
            background-color: white;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #eee;
            box-shadow: 0 2px 4px rgba(0,0,0,0.02);
            text-align: center;
        }
        div[data-testid="stMetricLabel"] { font-size: 0.9rem; color: #6c757d; }
        div[data-testid="stMetricValue"] { font-size: 1.6rem; font-weight: 700; color: #2d3436; }

        [data-testid="stForm"] {
            background-color: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            border: 1px solid #e0e0e0;
            padding: 20px;
        }
        
        h1, h2, h3, h4 { color: #2d3436; font-weight: 700; }
        .stDataFrame { border: 1px solid #f0f0f0; border-radius: 8px; }
    </style>
"""
st.markdown(custom_css, unsafe_allow_html=True)

MAX_WORKERS = 7
SCOPES = [
    'https://www.googleapis.com/auth/yt-analytics.readonly',
    'https://www.googleapis.com/auth/youtube.readonly'
]
DEFAULT_LIMIT_DATE = "2025-01-01"

# [ì§€ë„ìš©] ì£¼ìš” êµ­ê°€ ISO-2 -> ISO-3 ë§¤í•‘
ISO_MAPPING = {
    'KR': 'KOR', 'US': 'USA', 'JP': 'JPN', 'VN': 'VNM', 'TH': 'THA', 
    'ID': 'IDN', 'TW': 'TWN', 'PH': 'PHL', 'MY': 'MYS', 'IN': 'IND',
    'BR': 'BRA', 'MX': 'MEX', 'RU': 'RUS', 'GB': 'GBR', 'DE': 'DEU',
    'FR': 'FRA', 'CA': 'CAN', 'AU': 'AUS', 'HK': 'HKG', 'SG': 'SGP'
}

def render_md_allow_br(text: str) -> str:
    # 1) ì „ë¶€ escape í•´ì„œ HTML ì£¼ì… ì°¨ë‹¨
    escaped = html.escape(text or "")

    # 2) <br> / <br/> / <br /> ë§Œ ë‹¤ì‹œ ë³µì›
    escaped = re.sub(r"&lt;br\s*/?&gt;", "<br>", escaped, flags=re.IGNORECASE)
    return escaped
# endregion


# region [2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utilities)]
# ==========================================
def normalize_text(text):
    if not text: return ""
    return re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', text).lower()

PROMPT_FILE_1ST = "1ì°¨ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸.md"

def load_text_file(filename: str) -> str:
    base_dir = Path(__file__).resolve().parent  # ytan.pyê°€ ìˆëŠ” í´ë”
    path = base_dir / filename
    return path.read_text(encoding="utf-8")


def format_korean_number(num):
    if num == 0: return "0íšŒ"
    s = ""
    if num >= 100000000:
        eok = num // 100000000
        rem = num % 100000000
        s += f"{int(eok)}ì–µ "
        num = rem
    if num >= 10000:
        man = num // 10000
        rem = num % 10000
        s += f"{int(man)}ë§Œ "
        num = rem
    if num > 0:
        s += f"{int(num)}"
    return s.strip() + "íšŒ"

TRAFFIC_MAP = {
    'YT_SEARCH': 'ìœ íŠœë¸Œ ê²€ìƒ‰', 'RELATED_VIDEO': 'ì¶”ì²œ ë™ì˜ìƒ',
    'BROWSE_FEATURES': 'íƒìƒ‰ ê¸°ëŠ¥', 'EXT_URL': 'ì™¸ë¶€ ë§í¬',
    'NO_LINK_OTHER': 'ê¸°íƒ€', 'PLAYLIST': 'ì¬ìƒëª©ë¡',
    'VIDEO_CARD': 'ì¹´ë“œ/ìµœì¢…í™”ë©´', 'NOTIFICATION': 'ì•Œë¦¼'
}

def map_traffic_source(key):
    return TRAFFIC_MAP.get(key, key)

def parse_utc_to_kst_date(utc_str):
    try:
        dt_utc = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%SZ")
        dt_kst = dt_utc + timedelta(hours=9)
        return dt_kst.date()
    except: return None

def parse_duration_to_minutes(duration_str):
    if not duration_str: return 0.0
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(duration_str)
    if not match: return 0.0
    h, m, s = match.groups()
    total_sec = (int(h or 0) * 3600) + (int(m or 0) * 60) + (int(s or 0))
    return round(total_sec / 60, 1)

# [ìˆ˜ì •] Github ì—…ë¡œë“œ í•¨ìˆ˜ (í•œê¸€ ìëª¨ ë¶„ë¦¬ + 409 ì¶©ëŒ ìë™ ë³µêµ¬)
def upload_to_github(file_path, content_list):
    """
    ë¡œì»¬ ìºì‹œ ë°ì´í„°ë¥¼ GitHubì— ì €ì¥í•©ë‹ˆë‹¤.
    1. í•œê¸€ ìëª¨ ë¶„ë¦¬(NFC/NFD) ë¬¸ì œ í•´ê²°
    2. 409 Conflict(SHA ë¶ˆì¼ì¹˜) ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„
    """
    import unicodedata

    def normalize_str(s):
        return unicodedata.normalize('NFC', s) if s else ""

    try:
        if "github" not in st.secrets: 
            return False, "ì„¤ì • ì˜¤ë¥˜: secrets.tomlì— [github] ì„¹ì…˜ì´ ì—†ìŠµë‹ˆë‹¤."
        
        gh_token = st.secrets["github"]["token"]
        gh_repo = st.secrets["github"]["repo_name"]
        gh_branch = st.secrets["github"]["branch"]

        g = Github(gh_token)
        repo = g.get_repo(gh_repo)
        
        json_content = json.dumps(content_list, ensure_ascii=False, indent=2)
        commit_message = f"Update cache: {file_path} (via Streamlit App)"
        
        # 1. ì €ì¥ì†Œì˜ ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ëª… ì •ê·œí™” ë¹„êµë¥¼ ìœ„í•´)
        contents = repo.get_contents("", ref=gh_branch)
        
        target_sha = None
        target_path_on_git = file_path # ê¸°ë³¸ê°’ì€ ë¡œì»¬ íŒŒì¼ëª…
        
        # 2. ì •ê·œí™”ëœ ì´ë¦„ìœ¼ë¡œ íŒŒì¼ ì°¾ê¸°
        search_name = normalize_str(file_path)
        for c in contents:
            if normalize_str(c.path) == search_name:
                target_sha = c.sha
                target_path_on_git = c.path # ê¹ƒí—ˆë¸Œ ì‹¤ì¡´ ê²½ë¡œ
                break
        
        # 3. ì €ì¥ ì‹œë„ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        try:
            if target_sha:
                # ìˆ˜ì • (Update)
                repo.update_file(target_path_on_git, commit_message, json_content, target_sha, branch=gh_branch)
                return True, "Updated (SHA Found)"
            else:
                # ìƒì„± (Create)
                repo.create_file(file_path, commit_message, json_content, branch=gh_branch)
                return True, "Created (New File)"

        except GithubException as e:
            # ğŸš¨ [í•µì‹¬] 409 Conflict (SHA ë¶ˆì¼ì¹˜) ë°œìƒ ì‹œ ë³µêµ¬ ë¡œì§
            if e.status == 409:
                print("âš ï¸ 409 Conflict ë°œìƒ -> ìµœì‹  SHA ì¬ì¡°íšŒ í›„ 1íšŒ ì¬ì‹œë„")
                # í•´ë‹¹ íŒŒì¼ë§Œ ì½• ì§‘ì–´ì„œ ìµœì‹  ìƒíƒœë¥¼ ë‹¤ì‹œ ê°€ì ¸ì˜´
                fresh_file = repo.get_contents(target_path_on_git, ref=gh_branch)
                # ì¬ì‹œë„
                repo.update_file(target_path_on_git, commit_message, json_content, fresh_file.sha, branch=gh_branch)
                return True, "Updated (After 409 Retry)"
            else:
                # 409 ì™¸ì˜ ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ë³´ê³ 
                raise e

    except Exception as e:
        return False, str(e)
# endregion

# region [3. ì‹œê°í™” í•¨ìˆ˜ (Visualization)]
# ==========================================
def get_pyramid_chart_and_df(stats_dict, total_views):
    if not stats_dict: return None, None, ""
    
    age_order = ['age13-17', 'age18-24', 'age25-34', 'age35-44', 'age45-54', 'age55-64', 'age65-']
    display_labels = [label.replace('age', '') for label in age_order]
    
    male_data = defaultdict(float); female_data = defaultdict(float)
    table_rows = []; total_male = 0; total_female = 0

    for key, count in stats_dict.items():
        parts = key.split('_')
        if len(parts) != 2: continue
        age_group, gender = parts[0], parts[1]
        if gender not in ['male', 'female']: continue
        if age_group not in age_order: continue 
        
        pct = (count / total_views) * 100 if total_views > 0 else 0
        clean_age = age_group.replace('age', '')
        
        if gender == 'male':
            male_data[clean_age] += pct; total_male += pct
        elif gender == 'female':
            female_data[clean_age] += pct; total_female += pct
            
        table_rows.append({
            "ì—°ë ¹": clean_age, "ì„±ë³„": "ë‚¨" if gender=='male' else "ì—¬", 
            "ì¡°íšŒìˆ˜": int(count), "ë¹„ìœ¨": pct
        })

    male_vals = [male_data[l] for l in display_labels]
    female_vals = [female_data[l] for l in display_labels]
    male_vals_neg = [-v for v in male_vals] 

    fig = go.Figure()
    fig.add_trace(go.Bar(y=display_labels, x=male_vals_neg, name='ë‚¨ì„±', orientation='h',
        marker=dict(color='#5684D5'), text=[f"{v:.1f}%" if v>0 else "" for v in male_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ë‚¨ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, male_vals)]))
    fig.add_trace(go.Bar(y=display_labels, x=female_vals, name='ì—¬ì„±', orientation='h',
        marker=dict(color='#FF7675'), text=[f"{v:.1f}%" if v>0 else "" for v in female_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ì—¬ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, female_vals)]))
    
    max_val = max(max(male_vals) if male_vals else 0, max(female_vals) if female_vals else 0)
    max_range = max_val * 1.2 if max_val > 0 else 10

    fig.update_layout(
        barmode='overlay',
        xaxis=dict(tickvals=[-max_range, 0, max_range], ticktext=[f"{max_range:.0f}%", "0%", f"{max_range:.0f}%"], range=[-max_range, max_range]),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=10, r=10, t=30, b=10),
        height=300,
        paper_bgcolor='rgba(0,0,0,0)', 
        plot_bgcolor='rgba(0,0,0,0)'
    )
    
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df['ì—°ë ¹'] = pd.Categorical(df['ì—°ë ¹'], categories=display_labels, ordered=True)
        df = df.sort_values(['ì—°ë ¹', 'ì„±ë³„'])

    title_str = f"ğŸ‘¥ ì„±ë³„/ì—°ë ¹ (ë‚¨ {total_male:.1f}% vs ì—¬ {total_female:.1f}%)"
    return fig, df, title_str

def get_traffic_chart(traffic_dict):
    if not traffic_dict: return None
    sorted_t = sorted(traffic_dict.items(), key=lambda x: x[1], reverse=True)
    labels = []; values = []
    for k, v in sorted_t[:5]:
        labels.append(map_traffic_source(k)); values.append(v)
    if len(sorted_t) > 5:
        labels.append("ê¸°íƒ€"); values.append(sum(v for k,v in sorted_t[5:]))
    
    if not values: return None
        
    teal_colors = ['#00b894', '#00cec9', '#55efc4', '#81ecec', '#b2bec3', '#dfe6e9']
    fig = px.pie(names=labels, values=values, hole=0.5, color_discrete_sequence=teal_colors)
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_keyword_bar_chart(keyword_dict):
    if not keyword_dict: return None
    
    sorted_k = sorted(keyword_dict.items(), key=lambda x: x[1], reverse=True)[:10]
    if not sorted_k: return None
    
    words = [k[0] for k in sorted_k][::-1] 
    counts = [k[1] for k in sorted_k][::-1]
    
    fig = go.Figure(go.Bar(
        x=counts, y=words, orientation='h',
        marker=dict(color='#fdcb6e'),
        text=[f"{int(v):,}" for v in counts], textposition='auto'
    ))
    
    fig.update_layout(
        margin=dict(l=10, r=10, t=10, b=10),
        height=300,
        xaxis=dict(showticklabels=False, visible=False),
        yaxis=dict(tickfont=dict(size=13)),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_channel_share_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)
    labels = []; values = []
    top_list = sorted_ch[:5]
    others_sum = sum(ch['total_views'] for ch in sorted_ch[5:])
    
    for ch in top_list:
        labels.append(ch['channel_name']); values.append(ch['total_views'])
    if len(sorted_ch) > 5:
        labels.append("ê·¸ ì™¸ ì±„ë„"); values.append(others_sum)
    
    if sum(values) == 0: return None
        
    colors = []
    if highlight_channel:
        for label in labels:
            colors.append('#6c5ce7' if label == highlight_channel else '#dfe6e9')
    else:
        colors = ['#6c5ce7', '#a29bfe', '#8e44ad', '#9b59b6', '#d6a2e8', '#dfe6e9']

    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=0.5, marker=dict(colors=colors))])
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_engagement_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)[:10]
    names = [c['channel_name'] for c in sorted_ch]
    like_ratios = [(c['total_likes']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    share_ratios = [(c['total_shares']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    
    if not names: return None

    base_like_color = '#ff7675'; base_share_color = '#74b9ff'
    if highlight_channel:
        like_colors = [base_like_color if n == highlight_channel else '#dfe6e9' for n in names]
        share_colors = [base_share_color if n == highlight_channel else '#dfe6e9' for n in names]
    else:
        like_colors = base_like_color; share_colors = base_share_color

    fig = go.Figure()
    fig.add_trace(go.Bar(x=names, y=like_ratios, name='ì¢‹ì•„ìš”(%)', marker_color=like_colors))
    fig.add_trace(go.Bar(x=names, y=share_ratios, name='ê³µìœ (%)', marker_color=share_colors))

    fig.update_layout(
        barmode='group',
        margin=dict(l=20, r=20, t=10, b=40),
        height=300,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis=dict(title="ë¹„ìœ¨(%)", tickformat=".2f"),
        xaxis=dict(tickangle=-45),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_country_map(country_stats):
    if not country_stats: return None
    data = []
    for k, v in country_stats.items():
        iso3 = ISO_MAPPING.get(k, k)
        c_name = k 
        if k == 'KR': c_name = 'ëŒ€í•œë¯¼êµ­'
        elif k == 'US': c_name = 'ë¯¸êµ­'
        elif k == 'JP': c_name = 'ì¼ë³¸'
        elif k == 'VN': c_name = 'ë² íŠ¸ë‚¨'
        elif k == 'TH': c_name = 'íƒœêµ­'
        data.append({'iso_alpha': iso3, 'views': v, 'country': c_name, 'fmt_views': format_korean_number(v)})
    
    if not data: return None
    df_map = pd.DataFrame(data)
    df_kr = df_map[df_map['iso_alpha'] == 'KOR']
    df_others = df_map[df_map['iso_alpha'] != 'KOR']
    
    fig = go.Figure()
    if not df_others.empty:
        fig.add_trace(go.Choropleth(
            locations=df_others['iso_alpha'], z=df_others['views'], text=df_others['country'],
            customdata=df_others[['country', 'fmt_views']], colorscale='Teal',
            marker_line_color='#ffffff', marker_line_width=0.5, showscale=True,
            colorbar=dict(title="ì¡°íšŒìˆ˜", x=1.0, len=0.8),
            hovertemplate="<b>%{customdata[0]}</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))
    if not df_kr.empty:
        fig.add_trace(go.Choropleth(
            locations=df_kr['iso_alpha'], z=[1]*len(df_kr), text=df_kr['country'],
            customdata=df_kr[['country', 'fmt_views']], colorscale=[[0, '#5684D5'], [1, '#5684D5']], 
            marker_line_color='#ffffff', marker_line_width=1, showscale=False,
            hovertemplate="<b>%{customdata[0]} (Home)</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))

    fig.update_geos(
        showcountries=True, countrycolor="#E0E0E0",
        showcoastlines=True, coastlinecolor="#E0E0E0",
        showframe=False, projection_type='natural earth',
        fitbounds="locations" if df_map.empty else False,
        bgcolor='rgba(0,0,0,0)'
    )
    fig.update_layout(
        margin=dict(l=0, r=0, t=0, b=0), 
        height=400, 
        dragmode='pan',
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_daily_trend_chart(daily_stats, recent_gap=0):
    """
    daily_stats: Analytics API ì¼ë³„ ì¡°íšŒìˆ˜ (ë”•ì…”ë„ˆë¦¬)
    recent_gap: Data API(ì‹¤ì‹œê°„) ì´í•© - Analytics ì´í•©
    """
    if not daily_stats: return None
    
    # 1. ë‚ ì§œìˆœ ì •ë ¬ ë° ë°ì´í„° ì¶”ì¶œ
    sorted_dates = sorted(daily_stats.keys())
    daily_views = [daily_stats[d] for d in sorted_dates]
    
    # 2. ëˆ„ì  ì¡°íšŒìˆ˜(Cumulative) ê³„ì‚°
    cumulative_views = []
    current_sum = 0
    for v in daily_views:
        current_sum += v
        cumulative_views.append(current_sum)
    
    fig = go.Figure()
    
    # [A] í™•ì •ëœ ê³¼ê±° ë°ì´í„° (ì‹¤ì„ )
    fig.add_trace(go.Scatter(
        x=sorted_dates, 
        y=cumulative_views, 
        mode='lines+markers',
        name='ëˆ„ì  ì¡°íšŒìˆ˜ (í™•ì •)',
        line=dict(color='#6c5ce7', width=3),
        marker=dict(size=6),
        hovertemplate='%{x}<br>ëˆ„ì : %{y:,}íšŒ<extra></extra>'
    ))
    
    # [B] ì‹¤ì‹œê°„ êµ¬ê°„ ì—°ê²° (ìµœê·¼ 3ì¼ ì´ë‚´ì¼ ë•Œë§Œ)
    if recent_gap > 0 and sorted_dates:
        last_date_str = sorted_dates[-1]
        last_cum_val = cumulative_views[-1]
        
        today_dt = datetime.today()
        last_anl_dt = datetime.strptime(last_date_str, "%Y-%m-%d")
        
        # ì°¨ì´ê°€ 3ì¼ ì´ë‚´ì¸ ê²½ìš°ì—ë§Œ ì‹¤ì‹œê°„ ì ì„  ì—°ê²°
        if (today_dt - last_anl_dt).days <= 3:
            target_date_str = today_dt.strftime("%Y-%m-%d")
            final_total_val = last_cum_val + recent_gap
            
            if last_date_str != target_date_str:
                fig.add_trace(go.Scatter(
                    x=[last_date_str, target_date_str],
                    y=[last_cum_val, final_total_val],
                    mode='lines+markers',
                    name='ì‹¤ì‹œê°„ ì¶”ì´ (ìµœê·¼)',
                    line=dict(color='#ff7675', width=3, dash='dot'),
                    marker=dict(size=6, symbol='circle-open'),
                    hovertemplate=f'<b>ì‹¤ì‹œê°„(ì¶”ì •)</b><br>í˜„ì¬ ì´í•©: %{{y:,}}íšŒ<br>(ìµœê·¼ +{recent_gap:,}íšŒ ì¦ê°€)<extra></extra>'
                ))
                
                fig.add_annotation(
                    x=target_date_str, y=final_total_val,
                    text=f"Now (+{recent_gap:,})",
                    showarrow=True, arrowhead=2,
                    ax=0, ay=-20,
                    font=dict(color="#d63031", size=11, weight="bold")
                )

    # [Xì¶• ì¤‘ë³µ ë°©ì§€ ë¡œì§]
    # ì¡°íšŒ ê¸°ê°„ì´ ì§§ì„ ë•Œ(ì˜ˆ: 30ì¼ ì´ë‚´)ëŠ” ê°•ì œë¡œ '1ì¼ 1ëˆˆê¸ˆ(D1)'ì„ ì ìš©í•´ ì¤‘ë³µì„ ë§‰ìŠµë‹ˆë‹¤.
    # ê¸°ê°„ì´ ê¸¸ë©´(ì˜ˆ: 1ë…„) ìë™(Auto)ìœ¼ë¡œ ë‘¬ì•¼ ê²¹ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    dtick_setting = None
    if len(sorted_dates) <= 31: 
        dtick_setting = "D1"  # 1ì¼ 1ëˆˆê¸ˆ ê°•ì œ (ì¤‘ë³µ í•´ê²°)

    fig.update_layout(
        title="ğŸ“ˆ ëˆ„ì  ì¡°íšŒìˆ˜ ì„±ì¥ ì¶”ì´",
        margin=dict(l=20, r=20, t=40, b=20),
        height=350, 
        xaxis=dict(
            title=None, 
            tickformat="%m-%d", 
            dtick=dtick_setting  # [ìˆ˜ì •] ì—¬ê¸°ê°€ í•µì‹¬ì…ë‹ˆë‹¤!
        ),
        yaxis=dict(title="ì´ ì¡°íšŒìˆ˜", tickformat=","),
        hovermode="x unified",
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    return fig

def get_efficiency_scatter(video_details):
    if not video_details: return None
    df = pd.DataFrame(video_details)
    if df.empty: return None
    df = df[(df['duration_min'] > 0) & (df['avg_pct'].notnull()) & (df['avg_pct'] > 0)]
    if df.empty: return None
    
    fig = px.scatter(
        df, x='duration_min', y='avg_pct', 
        size='views', color='avg_pct',
        hover_name='title',
        labels={'duration_min': 'ì˜ìƒ ê¸¸ì´(ë¶„)', 'avg_pct': 'í‰ê·  ì§€ì†ë¥ (%)'},
        color_continuous_scale='Viridis'
    )
    fig.add_vline(x=df['duration_min'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    fig.add_hline(y=df['avg_pct'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    
    fig.update_layout(
        margin=dict(l=20, r=20, t=20, b=20),
        height=400,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=True, gridcolor='#eee'),
        yaxis=dict(showgrid=True, gridcolor='#eee')
    )
    return fig
# endregion


# region [4. API ë° ë°ì´í„° ì²˜ë¦¬ (API & Data Processing)]
# ==========================================
def get_creds_from_file(token_filename):
    creds = None
    if os.path.exists(token_filename):
        creds = google.oauth2.credentials.Credentials.from_authorized_user_file(token_filename, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(google.auth.transport.requests.Request())
                with open(token_filename, 'w') as token: token.write(creds.to_json())
            except: return None
        else: return None
    return creds

def process_sync_channel(token_file, limit_date, status_box, force_rescan):
    file_label = os.path.basename(token_file).replace("token_", "").replace(".json", "")
    creds = get_creds_from_file(token_file)
    if not creds: 
        status_box.error(f"âŒ [{file_label}] í† í° ì˜¤ë¥˜")
        return None
    try:
        youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
        ch_res = youtube.channels().list(part='snippet,contentDetails', mine=True).execute()
        if not ch_res['items']: 
            status_box.warning(f"âš ï¸ [{file_label}] ì •ë³´ ì—†ìŒ")
            return None
        ch_info = ch_res['items'][0]; ch_name = ch_info['snippet']['title']
        uploads_id = ch_info['contentDetails']['relatedPlaylists']['uploads']
        
        cache_file = f"cache_{token_file}"
        
        cached_videos = []
        cached_ids = set()
        
        if not force_rescan and os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f: cached_videos = json.load(f)
            cached_ids = {v['id'] for v in cached_videos}
            status_box.info(f"ğŸ”„ [{ch_name}] í™•ì¸ ì¤‘...")
        else: 
            if os.path.exists(cache_file):
                with open(cache_file, 'r', encoding='utf-8') as f: cached_videos = json.load(f)
            status_box.info(f"â³ [{ch_name}] ìŠ¤ìº” ì‹œì‘")
        
        new_videos = []; next_page_token = None; stop_scanning = False
        
        while not stop_scanning:
            req = youtube.playlistItems().list(part='snippet', playlistId=uploads_id, maxResults=50, pageToken=next_page_token)
            res = req.execute()
            for item in res['items']:
                vid = item['snippet']['resourceId']['videoId']
                title = item['snippet']['title']
                desc = item['snippet']['description']
                p_at = item['snippet']['publishedAt']
                
                if p_at < limit_date: stop_scanning = True; break
                
                if not force_rescan and vid in cached_ids: 
                    stop_scanning = True; break
                
                new_videos.append({'id': vid, 'title': title, 'date': p_at, 'description': desc})
            if len(new_videos) > 0 and len(new_videos) % 50 == 0:
                status_box.markdown(f"ğŸƒ **[{ch_name}]** +{len(new_videos)}")
            if not res.get('nextPageToken'): stop_scanning = True
            next_page_token = res.get('nextPageToken')
            if not next_page_token: stop_scanning = True
        
        if force_rescan:
            preserved_videos = [v for v in cached_videos if v['date'] < limit_date]
            final_list = new_videos + preserved_videos
        else:
            final_list = new_videos + cached_videos
        
        if new_videos or force_rescan:
            with open(cache_file, 'w', encoding='utf-8') as f: 
                json.dump(final_list, f, ensure_ascii=False, indent=2)
            
            # [ìˆ˜ì •] ì—…ë¡œë“œ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ì‹¤íŒ¨ ì‹œ í™”ë©´ì— ë„ìš°ê¸°!
            is_ok, error_msg = upload_to_github(cache_file, final_list)
            
            if is_ok:
                status_box.success(f"âœ… **[{ch_name}]** ê¹ƒí—ˆë¸Œ ì €ì¥ ì™„ë£Œ (+{len(new_videos)})")
            else:
                status_box.error(f"âš ï¸ **[{ch_name}]** ë¡œì»¬ë§Œ ì €ì¥ë¨ / ê¹ƒí—ˆë¸Œ ì‹¤íŒ¨:\n{error_msg}")
        else: 
            status_box.success(f"âœ… **[{ch_name}]** ìµœì‹ ")
        
        return {'creds': creds, 'name': ch_name, 'videos': final_list}
    except Exception as e:
        status_box.error(f"âŒ ì—ëŸ¬: {str(e)}")
        return {'error': str(e)}

def process_analysis_channel(channel_data, keyword, vid_start, vid_end, anl_start, anl_end):
    # (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼ - ìƒëµ ì—†ì´ ê·¸ëŒ€ë¡œ ìœ ì§€)
    creds = channel_data['creds']; videos = channel_data['videos']
    norm_keyword = normalize_text(keyword)
    target_ids = []
    id_map = {} 
    video_date_map = {}
    
    # 1. ì˜ìƒ í•„í„°ë§
    for v in videos:
        t_match = norm_keyword in normalize_text(v['title'])
        d_match = norm_keyword in normalize_text(v.get('description', ''))
        if not (t_match or d_match): continue
        
        v_dt_kst = parse_utc_to_kst_date(v['date'])
        if v_dt_kst and (vid_start <= v_dt_kst <= vid_end): 
            target_ids.append(v['id'])
            id_map[v['id']] = v['title']
            video_date_map[v['id']] = v['date']
            
    if not target_ids: return None
    
    yt_anl = googleapiclient.discovery.build('youtubeAnalytics', 'v2', credentials=creds)
    youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
    
    total_views = 0; total_likes = 0; total_shares = 0
    demo = defaultdict(float); traffic = defaultdict(float)
    country = defaultdict(float); daily = defaultdict(float)
    keywords_count = defaultdict(float)
    w_avg_sum = 0; v_for_avg = 0
    over_1m_count = 0 
    
    top_video_stats = []
    
    today_date = datetime.now().date()
    
    if isinstance(anl_end, str):
        anl_end_date = datetime.strptime(anl_end, "%Y-%m-%d").date()
    else:
        anl_end_date = anl_end

    if isinstance(anl_start, str):
        anl_start_date = datetime.strptime(anl_start, "%Y-%m-%d").date()
    else:
        anl_start_date = anl_start
        
    use_hybrid_logic = anl_end_date >= (today_date - timedelta(days=2))
    
    batch_size = 50 
    for i in range(0, len(target_ids), batch_size):
        batch_ids = target_ids[i : i + batch_size]
        vid_str = ",".join(batch_ids)
        
        anl_views_map = {}; anl_likes_map = {}; anl_retention_map = {}
        
        try:
            r_v = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views,likes,averageViewPercentage', dimensions='video', filters=f'video=={vid_str}').execute()
            if 'rows' in r_v and r_v['rows']:
                for r in r_v['rows']:
                    anl_views_map[r[0]] = r[1]; anl_likes_map[r[0]] = r[2]; anl_retention_map[r[0]] = r[3]
            
            r_b = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='shares', filters=f'video=={vid_str}').execute()
            if 'rows' in r_b and r_b['rows']: total_shares += r_b['rows'][0][0]

            r_d = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='viewerPercentage', dimensions='ageGroup,gender', filters=f'video=={vid_str}').execute()
            batch_total_view_anl = sum(anl_views_map.values())
            if 'rows' in r_d and r_d['rows']:
                for r in r_d['rows']: demo[f"{r[0]}_{r[1]}"] += batch_total_view_anl * (r[2] / 100)

            r_t = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceType', filters=f'video=={vid_str}').execute()
            if 'rows' in r_t and r_t['rows']:
                for r in r_t['rows']: traffic[r[0]] += r[1]
            
            try:
                r_k = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceDetail', filters=f'video=={vid_str};insightTrafficSourceType==YT_SEARCH', maxResults=15, sort='-views').execute()
                if 'rows' in r_k and r_k['rows']:
                    for r in r_k['rows']:
                        if r[0] != 'GOOGLE_SEARCH': keywords_count[r[0]] += r[1]
            except: pass

            r_c = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='country', filters=f'video=={vid_str}', maxResults=50).execute()
            if 'rows' in r_c and r_c['rows']:
                for r in r_c['rows']: country[r[0]] += r[1]

            r_day = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='day', filters=f'video=={vid_str}', sort='day').execute()
            if 'rows' in r_day and r_day['rows']:
                for r in r_day['rows']: daily[r[0]] += r[1]
        except: pass

        try:
            rt_res = youtube.videos().list(part='statistics,contentDetails', id=vid_str).execute()
            rt_stats_map = {}; rt_content_map = {}
            if 'items' in rt_res:
                for item in rt_res['items']:
                    rt_stats_map[item['id']] = item['statistics']
                    rt_content_map[item['id']] = item['contentDetails']

            for vid_id in batch_ids:
                v_date_str = video_date_map.get(vid_id)
                if not v_date_str: continue
                v_upload_dt = parse_utc_to_kst_date(v_date_str)
                if isinstance(v_upload_dt, datetime): v_upload_dt = v_upload_dt.date()

                a_v = anl_views_map.get(vid_id, 0)
                a_l = anl_likes_map.get(vid_id, 0)
                a_pct = anl_retention_map.get(vid_id, 0)
                
                stats = rt_stats_map.get(vid_id, {})
                rt_v = int(stats.get('viewCount', 0))
                rt_l = int(stats.get('likeCount', 0))
                
                final_v = 0; final_l = 0
                
                if use_hybrid_logic:
                    if v_upload_dt >= anl_start_date:
                        final_v = rt_v; final_l = rt_l
                    else:
                        deduct_end_date = anl_start_date - timedelta(days=1)
                        deduct_start_str = v_upload_dt.strftime("%Y-%m-%d")
                        deduct_end_str = deduct_end_date.strftime("%Y-%m-%d")
                        
                        past_views = 0; past_likes = 0; is_past_data_fetched = False
                        try:
                            if v_upload_dt <= deduct_end_date:
                                r_past = yt_anl.reports().query(ids='channel==MINE', startDate=deduct_start_str, endDate=deduct_end_str, metrics='views,likes', filters=f'video=={vid_id}').execute()
                                if 'rows' in r_past and r_past['rows']:
                                    past_views = r_past['rows'][0][0]
                                    past_likes = r_past['rows'][0][1]
                                    is_past_data_fetched = True
                                else:
                                    past_views = 0; past_likes = 0; is_past_data_fetched = True
                        except: is_past_data_fetched = False

                        if is_past_data_fetched:
                            final_v = max(0, rt_v - past_views)
                            final_l = max(0, rt_l - past_likes)
                        else:
                            final_v = a_v; final_l = a_l

                        if final_v == rt_v and a_v > 0 and past_views == 0:
                             final_v = a_v; final_l = a_l
                else:
                    final_v = a_v; final_l = a_l

                if final_v == 0 and rt_v > 0 and use_hybrid_logic:
                    final_v = rt_v; final_l = rt_l
                
                total_views += final_v; total_likes += final_l
                
                if final_v > 0 and a_pct > 0:
                    w_avg_sum += (final_v * a_pct)
                    v_for_avg += final_v
                
                if rt_v >= 1000000: over_1m_count += 1

                if final_v > 0:
                    top_video_stats.append({
                        'id': vid_id, 'title': id_map.get(vid_id, 'Unknown'),
                        'views': rt_v, 'likes': rt_l, 'period_views': final_v, 'period_likes': final_l,
                        'avg_pct': a_pct if a_pct > 0 else None,
                        'duration_min': parse_duration_to_minutes(rt_content_map.get(vid_id, {}).get('duration'))
                    })

        except: pass
        time.sleep(0.05)

    if not top_video_stats and total_views == 0: return None
    top_video_stats.sort(key=lambda x: x['period_views'], reverse=True)

    return {
        'channel_name': channel_data['name'], 'video_count': len(target_ids),
        'total_views': total_views, 'total_likes': total_likes, 'total_shares': total_shares,
        'avg_view_pct': (w_avg_sum/v_for_avg) if v_for_avg > 0 else 0,
        'demo_counts': demo, 'traffic_counts': traffic,
        'country_counts': country, 'daily_stats': daily,
        'keywords_counts': keywords_count,
        'top_video_stats': top_video_stats,
        'over_1m_count': over_1m_count 
    }
# endregion

# region [5. ë©”ì¸ UI ë° ì‹¤í–‰ ë¡œì§ (Main UI & Execution)]
# ==========================================
st.title("ğŸ“Š Drama YouTube Insight")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ›ï¸ ë°ì´í„° ê´€ë¦¬ ì„¼í„°")
    token_files = glob.glob("token_*.json")
    st.markdown("---")
    st.caption("ë°ì´í„° ë™ê¸°í™”")
    if st.button("ğŸ”„ ìµœì‹  ì˜ìƒ ì—…ë°ì´íŠ¸", type="primary", use_container_width=True):
        if not token_files: st.error("ì—°ë™ëœ í† í° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.session_state['channels_data'] = []
            st.write("--- ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘ ---")
            placeholders = {tf: st.empty() for tf in token_files}
            ready = []
            ctx = get_script_run_ctx()
            def sync_worker(tf, sb):
                add_script_run_ctx(ctx=ctx)
                return process_sync_channel(tf, DEFAULT_LIMIT_DATE, sb, False)
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                futs = {ex.submit(sync_worker, tf, placeholders[tf]): tf for tf in token_files}
                for f in as_completed(futs):
                    res = f.result()
                    if res and 'name' in res: ready.append(res)
            st.session_state['channels_data'] = ready
            if ready: st.success("ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
            
    st.markdown("---")
    with st.expander("ğŸ”’ ê³ ê¸‰ ê´€ë¦¬ì ì„¤ì •"):
        if 'admin_unlocked' not in st.session_state: st.session_state['admin_unlocked'] = False
        if not st.session_state['admin_unlocked']:
            if st.text_input("ë¹„ë°€ë²ˆí˜¸", type="password", key="pw_input") == "dima1234":
                st.session_state['admin_unlocked'] = True
                st.rerun()
        if st.session_state['admin_unlocked']:
            st.success("âœ… ê´€ë¦¬ì ëª¨ë“œ On")
            l_date = st.date_input("ìˆ˜ì§‘ ë§ˆì§€ë…¸ì„ ", value=pd.to_datetime(DEFAULT_LIMIT_DATE))
            if st.button("ğŸš¨ ì „ì²´ ì¬ìˆ˜ì§‘", type="secondary"):
                st.session_state['channels_data'] = []
                placeholders = {tf: st.empty() for tf in token_files}
                ready = []
                ctx = get_script_run_ctx()
                def deep_worker(tf, sb):
                    add_script_run_ctx(ctx=ctx)
                    return process_sync_channel(tf, l_date.strftime("%Y-%m-%d"), sb, True)
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                    futs = {ex.submit(deep_worker, tf, placeholders[tf]): tf for tf in token_files}
                    for f in as_completed(futs):
                        res = f.result()
                        if res and 'name' in res: ready.append(res)
                st.session_state['channels_data'] = ready
                if ready: st.success("ì™„ë£Œ!")
            if st.button("ğŸ”’ ì ê¸ˆ"):
                st.session_state['admin_unlocked'] = False
                st.rerun()

# --- ë©”ì¸ ---
if 'channels_data' not in st.session_state or not st.session_state['channels_data']:
    token_files = glob.glob("token_*.json")
    temp_data = []
    for tf in token_files:
        cf = f"cache_{tf}"
        if os.path.exists(cf):
             with open(cf, 'r', encoding='utf-8') as f:
                try:
                    vids = json.load(f); creds = get_creds_from_file(tf)
                    if creds:
                        lbl = os.path.basename(tf).replace("token_", "").replace(".json", "")
                        temp_data.append({'creds': creds, 'name': lbl, 'videos': vids})
                except: pass
    if temp_data: st.session_state['channels_data'] = temp_data
    else: st.info("ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤! ì‚¬ì´ë“œë°”ì—ì„œ [ìµœì‹  ì˜ìƒ ì—…ë°ì´íŠ¸]ë¥¼ ë¨¼ì € ì§„í–‰í•´ì£¼ì„¸ìš”.")

if 'channels_data' in st.session_state and st.session_state['channels_data']:
    data = st.session_state['channels_data']
    tv = sum(len(c['videos']) for c in data)
    
    st.markdown(f"""
    <div style='background-color:white; padding:10px 20px; border-radius:8px; border:1px solid #eee; margin-bottom:20px; display:flex; align-items:center; gap:10px;'>
        <span>âœ… <b>ì—°ë™ ìƒíƒœ:</b> ì±„ë„ <span style='color:#2980b9; font-weight:bold'>{len(data)}ê°œ</span></span>
        <span style='color:#ddd'>|</span>
        <span>ğŸ“ <b>DB ì˜ìƒ:</b> <span style='color:#2980b9; font-weight:bold'>{tv:,}ê°œ</span></span>
    </div>
    """, unsafe_allow_html=True)

    today = datetime.today()
    first_day = today.replace(day=1)

    with st.form("analysis_form"):
        st.subheader("ğŸ” í†µí•© ë¶„ì„ ì„¤ì •")
        c1, c2, c3 = st.columns([2, 1, 1])
        with c1: keyword = st.text_input("ë¶„ì„ IP", placeholder="ì˜ˆ: ëˆˆë¬¼ì˜ ì—¬ì™•")
        with c2: v_dates = st.date_input("ì˜ìƒ ì—…ë¡œë“œ ê¸°ê°„", value=(first_day, today))
        with c3: a_dates = st.date_input("ë°ì´í„° ì‚°ì¶œ ê¸°ê°„", value=(first_day, today))
        submit_btn = st.form_submit_button("ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

    if submit_btn:
        if isinstance(v_dates, tuple):
            v_start = v_dates[0]; v_end = v_dates[1] if len(v_dates)>1 else v_dates[0]
        else: v_start = v_end = v_dates
        if isinstance(a_dates, tuple):
            a_start = a_dates[0]; a_end = a_dates[1] if len(a_dates)>1 else a_dates[0]
        else: a_start = a_end = a_dates

        if not keyword.strip(): st.error("âš ï¸ ë¶„ì„ IPë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            # â¬‡ï¸ [ì¶”ê°€] ìƒˆ ë¶„ì„ ì‹œì‘ ì‹œ ì±—ë´‡ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ë‚´ìš© ì‚­ì œ)
            st.session_state["chat_active"] = False
            st.session_state["chat_history"] = []
            st.session_state["chat_context_comments"] = ""

            # 1. ì±—ë´‡ìš© ë‚ ì§œ ì €ì¥
            vs_str, ve_str = v_start.strftime("%Y-%m-%d"), v_end.strftime("%Y-%m-%d")
            st.session_state['analysis_dates'] = {'start': vs_str, 'end': ve_str}
            
            # 2. ë¶„ì„ìš© ë‚ ì§œ ì¤€ë¹„
            as_str = a_start.strftime("%Y-%m-%d"); ae_str = a_end.strftime("%Y-%m-%d")
            
            prog_bar = st.progress(0, text="ë°ì´í„° ë¶„ì„ ì¤‘...")
            ch_details_results = []
            
            ctx = get_script_run_ctx()
            def worker(cd, kw, vs, ve, ast, aet):
                add_script_run_ctx(ctx=ctx)
                return process_analysis_channel(cd, kw, vs, ve, ast, aet)

            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                futures = {ex.submit(worker, ch, keyword, v_start, v_end, as_str, ae_str): ch for ch in data}
                done = 0
                for f in as_completed(futures):
                    done += 1
                    prog_bar.progress(done/len(data), text=f"ì±„ë„ ë¶„ì„ ì¤‘... ({done}/{len(data)})")
                    res = f.result()
                    if res: ch_details_results.append(res)

            prog_bar.empty()
            
            # ê²°ê³¼ ì €ì¥ ë° ì•ˆë‚´
            if ch_details_results:
                st.session_state['analysis_raw_results'] = ch_details_results
                st.session_state['analysis_keyword'] = keyword
                st.success(f"ë¶„ì„ ì™„ë£Œ! ì´ {len(ch_details_results)}ê°œ ì±„ë„ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            else:
                st.session_state['analysis_raw_results'] = []
                st.warning(f"âš ï¸ '{keyword}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                           f"1. **ì˜ìƒ ì—…ë¡œë“œ ê¸°ê°„**ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. (í˜„ì¬ ì„¤ì •: {vs_str} ~ {ve_str})\n"
                           f"2. í‚¤ì›Œë“œê°€ ì •í™•í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”.")

    if 'analysis_raw_results' in st.session_state and st.session_state['analysis_raw_results']:
        raw_data = st.session_state['analysis_raw_results']
        current_kw = st.session_state['analysis_keyword']
        
        st.divider()
        st.markdown(f"### ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸: <span style='color:#2980b9;'>{current_kw}</span>", unsafe_allow_html=True)
        
        ch_names = sorted([d['channel_name'] for d in raw_data])
        sel_options = ["ì „ì²´ ì±„ë„ í•©ì‚°"] + ch_names
        
        c_sel_col, _ = st.columns([1, 2])
        with c_sel_col:
            selected_ch = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì±„ë„ ì„ íƒ", sel_options, label_visibility="collapsed")
        
        if selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°":
            target_data = raw_data
        else:
            target_data = [d for d in raw_data if d['channel_name'] == selected_ch]
            
        final_views = 0; final_likes = 0; final_shares = 0
        final_over_1m = 0; final_vid_count = 0
        final_stats = defaultdict(float); final_traffic = defaultdict(float)
        final_country = defaultdict(float); final_daily = defaultdict(float)
        final_keywords = defaultdict(float)
        w_avg_sum = 0; v_for_avg = 0
        final_top_videos = []
        
        for d in target_data:
            final_views += d['total_views']
            final_likes += d['total_likes']
            final_shares += d['total_shares']
            final_over_1m += d['over_1m_count']
            final_vid_count += d['video_count']
            
            if d['avg_view_pct'] > 0 and d['total_views'] > 0:
                w_avg_sum += (d['avg_view_pct'] * d['total_views'])
                v_for_avg += d['total_views']
            
            for k, v in d['demo_counts'].items(): final_stats[k] += v
            for k, v in d['traffic_counts'].items(): final_traffic[k] += v
            for k, v in d['country_counts'].items(): final_country[k] += v
            for k, v in d['daily_stats'].items(): final_daily[k] += v
            for k, v in d['keywords_counts'].items(): final_keywords[k] += v
            if 'top_video_stats' in d: final_top_videos.extend(d['top_video_stats'])
            
        final_avg_pct = (w_avg_sum / v_for_avg) if v_for_avg > 0 else 0
        
        anl_total_daily = sum(final_daily.values())
        recent_gap = final_views - anl_total_daily
        
        safe_anl_date = datetime.now() - timedelta(days=3)
        safe_str = safe_anl_date.strftime("%Y-%m-%d")
        
        if final_views > 0 or len(final_top_videos) > 0:
            st.caption(f"â„¹ï¸ **ë°ì´í„° ê¸°ì¤€**: ì¸êµ¬í†µê³„/ê²½ë¡œ ë“±ì€ **~{safe_str}** í™•ì •ì¹˜, **ì´ ì¡°íšŒìˆ˜/ì¢‹ì•„ìš” ë° ë¦¬ìŠ¤íŠ¸**ëŠ” **ì‹¤ì‹œê°„(Realtime)** ë°ì´í„°ì…ë‹ˆë‹¤.")

            m1, m2, m3, m4, m5, m6 = st.columns(6)
            m1.metric("ì´ ì¡°íšŒìˆ˜", f"{int(final_views):,}")
            m2.metric("ë¶„ì„ ì˜ìƒ", f"{final_vid_count:,}ê°œ")
            m3.metric("100ë§Œ+ ì˜ìƒ", f"{final_over_1m:,}ê°œ")
            m4.metric("í‰ê·  ì‹œì²­ì§€ì†ë¥ ", f"{final_avg_pct:.1f}%")
            m5.metric("ì´ ì¢‹ì•„ìš”", f"{int(final_likes):,}")
            m6.metric("ì´ ê³µìœ ", f"{int(final_shares):,}")
            st.write("")

            fig_demo, df_table, _ = get_pyramid_chart_and_df(final_stats, final_views)
            if fig_demo:
                c1, c2 = st.columns([1.6, 1])
                with c1:
                    st.markdown("##### ğŸ‘¥ ì„±ë³„/ì—°ë ¹ ë¶„í¬")
                    with st.container(border=True):
                        st.plotly_chart(fig_demo, use_container_width=True)
                with c2:
                    st.markdown("##### ğŸ“‹ ìƒì„¸ ë°ì´í„°")
                    with st.container(border=True):
                        if not df_table.empty:
                            df_disp = df_table.copy()
                            df_disp['ì¡°íšŒìˆ˜'] = df_disp['ì¡°íšŒìˆ˜'].apply(lambda x: f"{x:,}")
                            df_disp['ë¹„ìœ¨'] = df_disp['ë¹„ìœ¨'].apply(lambda x: f"{x:.1f}%")
                            st.dataframe(df_disp, use_container_width=True, hide_index=True, height=300)
                st.write("")

            fig_trend = get_daily_trend_chart(final_daily, recent_gap)
            if fig_trend:
                st.markdown("##### ğŸ“ˆ ì¼ë³„ ì¡°íšŒìˆ˜ ì¶”ì´ (Analytics + Realtime Gap)")
                with st.container(border=True):
                    st.plotly_chart(fig_trend, use_container_width=True)
                st.write("")

            st.markdown("##### ğŸ¥‡ ì¸ê¸° ì˜ìƒ TOP 100 (ê¸°ê°„ ë‚´ ì„±ê³¼ ê¸°ì¤€)")
            with st.container(border=True):
                if final_top_videos:
                    unique_vids_map = {v['id']: v for v in final_top_videos}
                    deduped_vids = list(unique_vids_map.values())
                    
                    top_vids = sorted(deduped_vids, key=lambda x: x['period_views'], reverse=True)[:100]
                    
                    df_top = pd.DataFrame(top_vids)
                    df_top['link'] = df_top['id'].apply(lambda x: f"https://youtu.be/{x}")
                    
                    df_show = df_top[['title', 'period_views', 'avg_pct', 'period_likes', 'link']].copy()
                    df_show.columns = ['ì œëª©', 'ì¡°íšŒìˆ˜', 'ì§€ì†ë¥ (%)', 'ì¢‹ì•„ìš”', 'ë°”ë¡œê°€ê¸°']
                    
                    df_show['ì¡°íšŒìˆ˜'] = df_show['ì¡°íšŒìˆ˜'].apply(lambda x: f"{int(x):,}")
                    df_show['ì¢‹ì•„ìš”'] = df_show['ì¢‹ì•„ìš”'].apply(lambda x: f"{int(x):,}")
                    
                    st.data_editor(
                        df_show,
                        column_config={
                            "ë°”ë¡œê°€ê¸°": st.column_config.LinkColumn(display_text="Watch ğŸ¬"),
                            "ì§€ì†ë¥ (%)": st.column_config.NumberColumn(format="%.1f%%"),
                        },
                        hide_index=True, use_container_width=True, disabled=True
                    )
                else: st.caption("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.write("")

            fig_traffic = get_traffic_chart(final_traffic)
            fig_keywords = get_keyword_bar_chart(final_keywords)
            
            if fig_traffic or fig_keywords:
                r2_1, r2_2 = st.columns(2)
                with r2_1:
                    if fig_traffic:
                        st.markdown("##### ğŸš¦ ìœ ì… ê²½ë¡œ Top 5")
                        with st.container(border=True):
                            st.plotly_chart(fig_traffic, use_container_width=True)
                with r2_2:
                    if fig_keywords:
                        st.markdown("##### ğŸ” Top 10 ê²€ìƒ‰ì–´ (SEO)")
                        with st.container(border=True):
                            st.plotly_chart(fig_keywords, use_container_width=True)
                st.write("")
            
            show_share = (selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°") and (len(raw_data) > 1)
            fig_share = get_channel_share_chart(raw_data, highlight_channel=None) if show_share else None
            if selected_ch != "ì „ì²´ ì±„ë„ í•©ì‚°" and len(raw_data) > 1:
                fig_share = get_channel_share_chart(raw_data, highlight_channel=selected_ch)
            
            fig_engage = get_engagement_chart(target_data, highlight_channel=selected_ch if selected_ch!="ì „ì²´ ì±„ë„ í•©ì‚°" else None)

            if fig_share or fig_engage:
                r3_1, r3_2 = st.columns(2)
                with r3_1:
                    if fig_share:
                        st.markdown("##### ğŸ† ì±„ë„ë³„ ì ìœ ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_share, use_container_width=True)
                with r3_2:
                    if fig_engage:
                        st.markdown("##### â¤ï¸ ì¢‹ì•„ìš”/ê³µìœ  ë¹„ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_engage, use_container_width=True)
                st.write("")

            fig_map = get_country_map(final_country)
            if fig_map:
                st.markdown("##### ğŸŒ ê¸€ë¡œë²Œ ì¡°íšŒìˆ˜ ë¶„í¬")
                with st.container(border=True):
                    st.plotly_chart(fig_map, use_container_width=True)
                st.write("")

            if final_top_videos:
                valid_scatter_vids = [v for v in final_top_videos if v.get('avg_pct') is not None and v.get('avg_pct') > 0]
                fig_scatter = get_efficiency_scatter(valid_scatter_vids)
                if fig_scatter:
                    st.markdown("##### âš¡ ì˜ìƒ íš¨ìœ¨ì„± ë§¤íŠ¸ë¦­ìŠ¤ (ê¸¸ì´ vs ì§€ì†ë¥ )")
                    st.caption("ìš°ìƒë‹¨ì— ìœ„ì¹˜í• ìˆ˜ë¡ ì˜ìƒë„ ê¸¸ê³  ëê¹Œì§€ ë³´ëŠ” ê³ íš¨ìœ¨ ì½˜í…ì¸ ì…ë‹ˆë‹¤.")
                    with st.container(border=True):
                        st.plotly_chart(fig_scatter, use_container_width=True)

        else:
            st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
# endregion

# region [6. AI ëŒ“ê¸€ ë¶„ì„ ë° ì±—ë´‡ (AI Chatbot Integration)]
# ==========================================
# ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì• ë„ë¦¬í‹±ìŠ¤ í•˜ë‹¨ì— í†µí•©ëœ ì±—ë´‡ ëª¨ë“ˆì…ë‹ˆë‹¤.
# ê¸°ì¡´ ytcc_chatbot.pyì˜ í•µì‹¬ ê¸°ëŠ¥ì„ ì´ì‹í–ˆìŠµë‹ˆë‹¤.
# ==========================================

# [ì„¤ì •] API í‚¤ ë¡œë“œ (secrets.tomlì— ì„¤ì • í•„ìš”)
GEMINI_API_KEYS = st.secrets.get("GEMINI_API_KEYS", [])
YT_PUBLIC_KEYS = st.secrets.get("YT_API_KEYS", [])
GEMINI_MODEL = "gemini-2.5-flash-lite"

# [í•¨ìˆ˜] Gemini í˜¸ì¶œ (Rotating Key)
def call_gemini_integrated(system_prompt, user_prompt):
    if not GEMINI_API_KEYS: return "âš ï¸ ì„¤ì • ì˜¤ë¥˜: 'GEMINI_API_KEYS'ê°€ secretsì— ì—†ìŠµë‹ˆë‹¤."
    
    import google.generativeai as genai
    from google.generativeai.types import HarmCategory, HarmBlockThreshold

    # ì•ˆì „ ì„¤ì • í•´ì œ
    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    }

    # í‚¤ ìˆœí™˜ ì‹œë„
    for key in GEMINI_API_KEYS:
        try:
            genai.configure(api_key=key)
            model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=system_prompt)
            resp = model.generate_content(
                user_prompt, 
                safety_settings=safety_settings,
                request_options={"timeout": 120}
            )
            if resp and hasattr(resp, 'text'):
                return resp.text
        except Exception as e:
            continue # ë‹¤ìŒ í‚¤ ì‹œë„
            
    return "âŒ AI ì‘ë‹µ ìƒì„± ì‹¤íŒ¨ (API í‚¤ í• ë‹¹ëŸ‰ ì´ˆê³¼ ë˜ëŠ” ì˜¤ë¥˜)"

# [í•¨ìˆ˜] UGC ì˜ìƒ ê²€ìƒ‰ (ê¸°ê°„ í•„í„° + OST ì œì™¸ + ì¤‘ë³µ ì œì™¸)
def search_ugc_videos(keyword, existing_ids, start_date=None, end_date=None, max_search=50):
    if not YT_PUBLIC_KEYS: return []
    
    youtube_pub = None
    for key in YT_PUBLIC_KEYS:
        try:
            youtube_pub = googleapiclient.discovery.build('youtube', 'v3', developerKey=key)
            # í‚¤ ìœ íš¨ì„± í…ŒìŠ¤íŠ¸
            youtube_pub.search().list(part='id', q='test', maxResults=1).execute()
            break
        except: continue
    
    if not youtube_pub: return []

    ugc_ids = []
    try:
        # ë‚ ì§œ í¬ë§· ë³€í™˜ (YYYY-MM-DD -> RFC 3339 í¬ë§·)
        p_after = f"{start_date}T00:00:00Z" if start_date else None
        p_before = f"{end_date}T23:59:59Z" if end_date else None

        # í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ (ê¸°ê°„ í•„í„° ì ìš©)
        search_res = youtube_pub.search().list(
            q=keyword, 
            part="id,snippet", 
            type="video", 
            maxResults=max_search, 
            order="relevance",
            publishedAfter=p_after,
            publishedBefore=p_before
        ).execute()
        
        existing_set = set(existing_ids)
        
        for item in search_res.get('items', []):
            vid = item['id']['videoId']
            title = item['snippet']['title']
            
            # [í•„í„°ë§] ì´ë¯¸ ìˆ˜ì§‘ëœ ì˜ìƒì´ê±°ë‚˜, ì œëª©ì— 'OST'ê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
            if vid in existing_set: continue
            if "ost" in title.lower(): continue  # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ OST ì œì™¸
            
            ugc_ids.append(vid)
                
    except Exception as e:
        # ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì¡°ìš©íˆ ë„˜ê¹€ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
        pass
        
    return ugc_ids

# [í•¨ìˆ˜] ëŒ“ê¸€ ìˆ˜ì§‘ (ì œí•œ ëŒ€í­ ì™„í™”)
def collect_comments_fast(video_ids, max_comments=10000, max_workers=None):
    """
    - ì˜ìƒë³„ commentThreads.list(ìµœëŒ€ 100ê°œ) í˜¸ì¶œì„ ThreadPoolë¡œ ë³‘ë ¬í™”
    - Streamlit UI ê°ì²´(st.*)ëŠ” ìŠ¤ë ˆë“œ ì•ˆì—ì„œ ì ˆëŒ€ ê±´ë“œë¦¬ì§€ ì•ŠìŒ
    """
    if not YT_PUBLIC_KEYS or not video_ids:
        return ""

    # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
    seen = set()
    video_ids = [v for v in video_ids if not (v in seen or seen.add(v))]

    # ì›Œì»¤ ìˆ˜: ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ 429/ì¿¼í„°/ì†ë„ ì—­íš¨ê³¼ ë‚  ìˆ˜ ìˆìŒ
    # ê¸°ë³¸ê°’: MAX_WORKERS(íŒŒì¼ ìƒìˆ˜) ë˜ëŠ” 8 ì¤‘ ì‘ì€ ê°’ ê¶Œì¥
    if max_workers is None:
        max_workers = min(8, max(1, MAX_WORKERS))  # MAX_WORKERSëŠ” íŒŒì¼ ìƒìˆ˜(í˜„ì¬ 7)
    max_workers = max(1, min(max_workers, len(video_ids)))

    # ìŠ¤ë ˆë“œ ë¡œì»¬ë¡œ "í‚¤ë³„ youtube client" ìºì‹œ (build ë¹„ìš© ì ˆê°)
    import threading
    _tls = threading.local()

    def _get_client(dev_key: str):
        if not hasattr(_tls, "clients"):
            _tls.clients = {}
        if dev_key not in _tls.clients:
            # cache_discovery=False: discovery ìºì‹œ/IO ì¤„ì—¬ì„œ build ì¡°ê¸ˆ ê°€ë²¼ì›Œì§
            _tls.clients[dev_key] = googleapiclient.discovery.build(
                "youtube", "v3", developerKey=dev_key, cache_discovery=False
            )
        return _tls.clients[dev_key]

    def _fetch_one(vid: str, dev_key: str):
        out = []
        try:
            yt = _get_client(dev_key)
            req = yt.commentThreads().list(
                part="snippet",
                videoId=vid,
                maxResults=100,                 # 1íšŒ ìµœëŒ€
                textFormat="plainText",
                order="relevance"
            )
            res = req.execute()
            for item in res.get("items", []):
                snip = item["snippet"]["topLevelComment"]["snippet"]
                text = (snip.get("textDisplay") or "").replace("\n", " ")
                likes = int(snip.get("likeCount") or 0)
                out.append((likes, text))
        except Exception:
            # ëŒ“ê¸€ ë¹„í™œì„±/ì°¨ë‹¨/ê¶Œí•œ/ì¼ì‹œì  ì—ëŸ¬ ë“±ì€ ê·¸ëƒ¥ ìŠ¤í‚µ
            pass
        return out

    comments = []
    total = 0

    # ë°°ì¹˜ ë‹¨ìœ„ë¡œ futureë¥¼ ë§Œë“¤ë©´ ë©”ëª¨ë¦¬/ìº”ìŠ¬ì´ ì¢€ ë” ìˆ˜ì›”í•¨
    batch_size = max_workers * 6

    def _chunks(lst, n):
        for i in range(0, len(lst), n):
            yield lst[i:i+n]

    submit_counter = 0

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        for batch in _chunks(video_ids, batch_size):
            futures = []
            for vid in batch:
                dev_key = YT_PUBLIC_KEYS[submit_counter % len(YT_PUBLIC_KEYS)]
                submit_counter += 1
                futures.append(ex.submit(_fetch_one, vid, dev_key))

            for fut in as_completed(futures):
                rows = fut.result() or []
                for likes, text in rows:
                    comments.append(f"[â™¥{likes}] {text}")
                    total += 1
                    if total >= max_comments:
                        return "\n".join(comments)

    return "\n".join(comments)


# ==========================================
# [UI] AI ëŒ€í™”í•˜ê¸° ì„¹ì…˜
# ==========================================
st.divider()

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_active" not in st.session_state: st.session_state["chat_active"] = False
if "chat_history" not in st.session_state: st.session_state["chat_history"] = []
if "chat_context_comments" not in st.session_state: st.session_state["chat_context_comments"] = ""

# ë²„íŠ¼: ë¶„ì„ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ í™œì„±í™”
if 'analysis_raw_results' in st.session_state and st.session_state['analysis_raw_results']:
    
    # 1. ì§„ì… ë²„íŠ¼ (ì±—ë´‡ ë¹„í™œì„± ìƒíƒœì¼ ë•Œ)
    if not st.session_state["chat_active"]:
        st.subheader("ğŸ¤– AI ì‹¬ì¸µ ë¶„ì„")
        st.caption("í˜„ì¬ ë¶„ì„ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ UGC(ì™¸ë¶€ ë°˜ì‘)ê¹Œì§€ í¬í•¨í•˜ì—¬ AIì™€ ëŒ€í™”í•©ë‹ˆë‹¤.")
        
        if st.button("ğŸ’¬ AIì™€ ëŒ€í™”í•˜ê¸° (UGC í¬í•¨)", type="primary"):
            with st.spinner("ğŸ”„ ì™¸ë¶€ ì—¬ë¡  ìˆ˜ì§‘ ë° AI ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
                # A. ë°ì´í„° ì¤€ë¹„
                current_kw = st.session_state.get('analysis_keyword', '')
                raw_results = st.session_state['analysis_raw_results']
                
                # [ì ìš©] ì €ì¥í•´ë‘” ë‚ ì§œ êº¼ë‚´ì˜¤ê¸°
                dates = st.session_state.get('analysis_dates', {})
                v_start = dates.get('start')
                v_end = dates.get('end')
                
                # B. ì±„ë„ ë‚´ ì˜ìƒ ID ì¶”ì¶œ (ì œí•œ ì—†ìŒ!)
                channel_vids = []
                for ch in raw_results:
                    if 'top_video_stats' in ch:
                        channel_vids.extend([v['id'] for v in ch['top_video_stats']])
                
                # C. UGC(ì™¸ë¶€) ì˜ìƒ ì¶”ê°€ ê²€ìƒ‰ (ê¸°ê°„ í•„í„° + OST ì œì™¸ ì ìš©)
                ugc_vids = search_ugc_videos(current_kw, channel_vids, start_date=v_start, end_date=v_end)
                
                # D. ëŒ“ê¸€ ìˆ˜ì§‘ (ì±„ë„ ì˜ìƒ ì „ì²´ + UGC ì˜ìƒ ì „ì²´)
                # [ìˆ˜ì •] ìŠ¬ë¼ì´ì‹±([:20]) ì œê±° -> ì „ì²´ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©
                target_vids = channel_vids + ugc_vids
                
                # [ìˆ˜ì •] ë„‰ë„‰í•˜ê²Œ 10,000ê°œê¹Œì§€ ìˆ˜ì§‘ ìš”ì²­
                collected_text = collect_comments_fast(target_vids, max_comments=10000)
                st.session_state["chat_context_comments"] = collected_text
                
                # E. í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ì „ë¬¸ì„± ê°•í™” ë° ì •í˜•í™”ëœ í¬ë§· ì ìš©)
                sys_prompt = ( load_text_file(PROMPT_FILE_1ST))

                
                video_info_str = f"ì±„ë„ ê³µì‹ ì˜ìƒ {len(channel_vids)}ê°œ + UGC(ì™¸ë¶€) ì˜ìƒ {len(ugc_vids)}ê°œ"
                if v_start and v_end:
                    video_info_str += f" (ê¸°ê°„: {v_start} ~ {v_end})"

                user_payload = (
                    f"ë¶„ì„ ì£¼ì œ(í‚¤ì›Œë“œ): {current_kw}\n"
                    f"ë¶„ì„ ëŒ€ìƒ: {video_info_str}\n"
                    f"ëŒ“ê¸€ ë°ì´í„° ìƒ˜í”Œ:\n{collected_text[:150000]}..." # 15ë§Œì ì œí•œ (ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™œìš©)
                )
                
                # F. ì²« ë¶„ì„ ì‹¤í–‰
                ai_response = call_gemini_integrated(sys_prompt, user_payload)
                
                # G. ì„¸ì…˜ ì €ì¥ ë° ì±—ë´‡ í™œì„±í™”
                st.session_state["chat_history"].append({"role": "assistant", "content": ai_response})
                st.session_state["chat_active"] = True
                st.rerun()

    # 2. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ (í™œì„±í™” ì‹œ)
    else:
        st.subheader(f"ğŸ’¬ AI ì±—ë´‡: {st.session_state.get('analysis_keyword', 'ë¶„ì„')}")
        
        # ë‹«ê¸° ë²„íŠ¼
        if st.button("âŒ ëŒ€í™” ì¢…ë£Œ ë° ì´ˆê¸°í™”"):
            st.session_state["chat_active"] = False
            st.session_state["chat_history"] = []
            st.session_state["chat_context_comments"] = ""
            st.rerun()
            
        # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
        chat_container = st.container(border=True)
        with chat_container:
            for msg in st.session_state["chat_history"]:
                with st.chat_message(msg["role"]):
                    if msg["role"] == "assistant":
                        st.markdown(render_md_allow_br(msg["content"]), unsafe_allow_html=True)
                    else:
                        st.markdown(msg["content"]) 
        
        # í›„ì† ì§ˆë¬¸ ì…ë ¥
        if prompt := st.chat_input("ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš” (ì˜ˆ: ì£¼ì—° ë°°ìš° ì—°ê¸° ë°˜ì‘ì€ ì–´ë•Œ?)"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            st.session_state["chat_history"].append({"role": "user", "content": prompt})
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(prompt)
            
            # AI ì‘ë‹µ ìƒì„±
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                context_comments = st.session_state.get("chat_context_comments", "")
                
                sys_prompt_followup = (
                    "ì—­í• : ë„ˆëŠ” ì•ì„œ ë¶„ì„í•œ ëŒ“ê¸€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì¡°ì‚¬ê´€ì´ë‹¤.\n"
                    "ê·œì¹™:\n"
                    "1. ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ëŒ“ê¸€ë§Œ í•„í„°ë§í•˜ì—¬ ê·¼ê±°ë¡œ ì œì‹œí•˜ë¼.\n"
                    "2. ë‡Œí”¼ì…œ(ì¶”ì¸¡)ì„ ìì œí•˜ê³  ë°ì´í„°ì— ê¸°ë°˜í•´ ë‹µí•˜ë¼.\n"
                    "3. ëŒ“ê¸€ ì¸ìš© ì‹œ ìš•ì„¤ì€ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬í•˜ë¼.\n"
                )
                
                # ì´ì „ ëŒ€í™” ë§¥ë½ í¬í•¨ (ìµœê·¼ 2í„´)
                conversation_context = ""
                for m in st.session_state["chat_history"][-4:]:
                    conversation_context += f"[{m['role']}]: {m['content']}\n"

                payload_followup = (
                    f"ëŒ“ê¸€ ë°ì´í„°:\n{context_comments[:150000]}\n\n"
                    f"ì´ì „ ëŒ€í™”:\n{conversation_context}\n\n"
                    f"ì‚¬ìš©ì ì§ˆë¬¸: {prompt}"
                )
                
                ai_reply = call_gemini_integrated(sys_prompt_followup, payload_followup)
                
                st.session_state["chat_history"].append({"role": "assistant", "content": ai_reply})
                st.rerun()

else:
    # ë°ì´í„°ê°€ ì—†ì„ ë•Œ ì•ˆë‚´ ë¬¸êµ¬
    st.info("â„¹ï¸ AI ëŒ€í™” ê¸°ëŠ¥ì€ ìƒë‹¨ì—ì„œ [ë¶„ì„ ì‹œì‘]ì„ ì™„ë£Œí•œ í›„ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
# endregion