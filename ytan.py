import streamlit as st
import os
import glob
import json
import time
import re
import hashlib
import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import google.oauth2.credentials
import googleapiclient.discovery
import google.auth.transport.requests
import extra_streamlit_components as stx 
import google.generativeai as genai
from googleapiclient.errors import HttpError
import html
import html as _html
from pathlib import Path
from streamlit.components.v1 import html as st_html

from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
from datetime import datetime, timedelta
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore
from github import Github, GithubException # PyGithub

# region [1. ì„¤ì • ë° ìƒìˆ˜ (Config & Constants)]
# ==========================================
import streamlit as st
import os
import glob
import json
import time
import re
import hashlib
import datetime
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
import google.oauth2.credentials
import googleapiclient.discovery
import google.auth.transport.requests
import extra_streamlit_components as stx 
import google.generativeai as genai
from googleapiclient.errors import HttpError
import html
import html as _html
from pathlib import Path
from streamlit.components.v1 import html as st_html

from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from streamlit.runtime.scriptrunner import add_script_run_ctx, get_script_run_ctx
from datetime import datetime, timedelta
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

# [ì¶”ê°€] ìŠ¤ì¼€ì¤„ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import pytz

# [ë³€ê²½] ì‚¬ì´ë“œë°” ê¸°ë³¸ ìƒíƒœ: 'collapsed' (ì ‘í˜)
st.set_page_config(
    page_title="Drama YouTube Insight", 
    page_icon="ğŸ“Š",
    layout="wide", 
    initial_sidebar_state="collapsed" 
)
# endregion


# region [1-1. ì…ì¥ê²Œì´íŠ¸ (ë³´ì•ˆ ì¸ì¦)]
# ==========================================
# Dashboard.pyì—ì„œ ì´ì‹ëœ ì¿ í‚¤/ë¹„ë°€ë²ˆí˜¸ ì¸ì¦ ë¡œì§
# ==========================================

def _rerun():
    """ìŠ¤íŠ¸ë¦¼ë¦¿ ë²„ì „ í˜¸í™˜ ë¦¬ëŸ° í•¨ìˆ˜"""
    if hasattr(st, "rerun"):
        st.rerun()
    else:
        st.experimental_rerun()

def get_cookie_manager():
    # ì¿ í‚¤ ë§¤ë‹ˆì €ëŠ” í‚¤(Key)ê°€ ê³ ìœ í•´ì•¼ í•¨
    return stx.CookieManager(key="yt_auth_cookie_manager")

def _hash_password(password: str) -> str:
    return hashlib.sha256(str(password).encode()).hexdigest()

def check_password_with_cookie() -> bool:
    """
    1. Secrets ë¹„ë°€ë²ˆí˜¸ í™•ì¸
    2. ì¿ í‚¤(ê³¼ê±° ë¡œê·¸ì¸) í™•ì¸
    3. ì„¸ì…˜(í˜„ì¬ ë¡œê·¸ì¸) í™•ì¸
    4. ì‹¤íŒ¨ ì‹œ ë¡œê·¸ì¸ì°½ ë„ìš°ê³  False ë°˜í™˜ -> ì•± ì¤‘ë‹¨
    """
    cookie_manager = get_cookie_manager()
    
    # Secretsì—ì„œ ë¹„ë°€ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸° (ì—†ìœ¼ë©´ ì—ëŸ¬)
    # secrets.toml íŒŒì¼ì— [general] ì„¹ì…˜ í˜¹ì€ ìµœìƒë‹¨ì— DASHBOARD_PASSWORD = "..." ê°€ ìˆì–´ì•¼ í•¨
    secret_pwd = st.secrets.get("DASHBOARD_PASSWORD")
    if not secret_pwd:
        # í˜¸í™˜ì„±ì„ ìœ„í•´ general ì„¹ì…˜ë„ ì²´í¬
        if "general" in st.secrets:
            secret_pwd = st.secrets["general"].get("DASHBOARD_PASSWORD")
            
    if not secret_pwd:
        st.error("ğŸ”’ ì„¤ì • ì˜¤ë¥˜: Secretsì— 'DASHBOARD_PASSWORD'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()
        
    hashed_secret = _hash_password(str(secret_pwd))
    
    # ì¿ í‚¤ ì½ê¸°
    cookies = cookie_manager.get_all()
    COOKIE_NAME = "yt_dashboard_auth"
    current_token = cookies.get(COOKIE_NAME)
    
    # ì¸ì¦ ê²€ì‚¬
    is_cookie_valid = (current_token == hashed_secret)
    is_session_valid = st.session_state.get("auth_success", False)
    
    if is_cookie_valid or is_session_valid:
        if is_cookie_valid and not is_session_valid:
            st.session_state["auth_success"] = True
        return True

    # ë¡œê·¸ì¸ UI
    st.markdown("#### ğŸ”’ Access Restricted")
    st.caption("ê´€ê³„ì ì™¸ ì ‘ê·¼ì´ ì œí•œëœ í˜ì´ì§€ì…ë‹ˆë‹¤.")
    
    col1, col2 = st.columns([1, 2])
    with col1:
        input_pwd = st.text_input("Password", type="password", key="login_pw_input")
        login_btn = st.button("Login", type="primary", use_container_width=True)

    if login_btn:
        if _hash_password(input_pwd) == hashed_secret:
            # ì¿ í‚¤ êµ½ê¸° (1ì¼ ìœ íš¨)
            expires = datetime.now() + timedelta(days=1)
            cookie_manager.set(COOKIE_NAME, hashed_secret, expires_at=expires)
            
            st.session_state["auth_success"] = True
            st.success("âœ… ì¸ì¦ ì„±ê³µ")
            time.sleep(0.5)
            _rerun()
        else:
            st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            
    return False

# ğŸ›‘ [ì¤‘ìš”] ì—¬ê¸°ì„œ ì¸ì¦ ì‹¤íŒ¨ ì‹œ ì•± ì‹¤í–‰ì„ ë©ˆì¶¤
if not check_password_with_cookie():
    st.stop()

# ==========================================
# ì¸ì¦ í†µê³¼ í›„ ì‹¤í–‰ë˜ëŠ” ì˜ì—­
# ==========================================
# endregion


# region [1-2. ë°°í¬ í™˜ê²½ ì„¤ì • (Secrets ë³µì›)]
# ==========================================
# Streamlit Secretsì— ì €ì¥ëœ í† í° ì •ë³´ë¥¼ ì½ì–´ ë¡œì»¬ íŒŒì¼ë¡œ ë³µì›
if "tokens" in st.secrets:
    for file_name, content in st.secrets["tokens"].items():
        if not file_name.endswith(".json"):
            file_name += ".json"
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
        if not os.path.exists(file_name):
            with open(file_name, "w", encoding='utf-8') as f:
                f.write(content)
# endregion


# region [1-3. ë””ìì¸ ë° ìƒìˆ˜]
# ==========================================
# UI ë””ìì¸ CSS
custom_css = """
    <style>
        /* í—¤ë” íˆ¬ëª…í™” */
        header[data-testid="stHeader"] { background: transparent; }
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        [data-testid="stDecoration"] {display: none;}
        
        .block-container { padding-top: 1rem; padding-bottom: 3rem; }
        .stApp { background-color: #f8f9fa; }

        /* ì¹´ë“œ ë° ë©”íŠ¸ë¦­ ìŠ¤íƒ€ì¼ */
        div[data-testid="stMetric"] {
            background-color: white;
            padding: 15px;
            border-radius: 10px;
            border: 1px solid #eee;
            box-shadow: 0 2px 4px rgba(0,0,0,0.02);
            text-align: center;
        }
        div[data-testid="stMetricLabel"] { font-size: 0.9rem; color: #6c757d; }
        div[data-testid="stMetricValue"] { font-size: 1.6rem; font-weight: 700; color: #2d3436; }

        [data-testid="stForm"] {
            background-color: white;
            border-radius: 12px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.05);
            border: 1px solid #e0e0e0;
            padding: 20px;
        }
        
        h1, h2, h3, h4 { color: #2d3436; font-weight: 700; }
        .stDataFrame { border: 1px solid #f0f0f0; border-radius: 8px; }
    </style>
"""

REPORT_CSS = """
<style>
.yt-report { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
.yt-report .badge { display:inline-block; padding:4px 10px; border-radius:999px; background:#f1f5f9; margin-right:6px; font-size:12px; }
.yt-report .grid.two { display:grid; grid-template-columns: 1fr 1fr; gap:12px; }
.yt-report .card { border:1px solid #e5e7eb; border-radius:14px; padding:14px; background:#fff; box-shadow: 0 6px 18px rgba(0,0,0,0.04); margin-bottom:12px; }
.yt-report table { width:100%; border-collapse: collapse; }
.yt-report th { text-align:left; padding:10px; background:#f8fafc; border:1px solid #e5e7eb; width: 180px; vertical-align: top; }
.yt-report td { padding:10px; border:1px solid #e5e7eb; vertical-align: top; }
.yt-report ul { margin: 0; padding-left: 18px; }
.yt-report .muted { color:#64748b; font-weight: normal; }
.yt-report .note { color:#64748b; margin-top:8px; font-size:12px; }
.yt-report .quote { border-left: 3px solid #cbd5e1; padding:8px 10px; margin: 6px 0; background:#f8fafc; border-radius:10px; }
</style>
"""
st.markdown(REPORT_CSS, unsafe_allow_html=True)

st.markdown(custom_css, unsafe_allow_html=True)

MAX_WORKERS = 7
SCOPES = [
    'https://www.googleapis.com/auth/yt-analytics.readonly',
    'https://www.googleapis.com/auth/youtube.readonly'
]
DEFAULT_LIMIT_DATE = "2025-01-01"

# [ì§€ë„ìš©] ì£¼ìš” êµ­ê°€ ISO-2 -> ISO-3 ë§¤í•‘
ISO_MAPPING = {
    'KR': 'KOR', 'US': 'USA', 'JP': 'JPN', 'VN': 'VNM', 'TH': 'THA', 
    'ID': 'IDN', 'TW': 'TWN', 'PH': 'PHL', 'MY': 'MYS', 'IN': 'IND',
    'BR': 'BRA', 'MX': 'MEX', 'RU': 'RUS', 'GB': 'GBR', 'DE': 'DEU',
    'FR': 'FRA', 'CA': 'CAN', 'AU': 'AUS', 'HK': 'HKG', 'SG': 'SGP'
}

def render_md_allow_br(text: str) -> str:
    """
    - ê¸°ë³¸ì€ ì•ˆì „í•˜ê²Œ escape + <br>ë§Œ í—ˆìš©
    - ë‹¨, <!--REPORT_START--> ~ <!--REPORT_END--> í¬í•¨ ì‹œ í•´ë‹¹ êµ¬ê°„ì€ raw HTMLë¡œ ë Œë”ë§
    """
    raw = (text or "").strip()

    # 0) ì½”ë“œíœìŠ¤ ì œê±° (```html ... ``` í˜•íƒœ ëŒ€ë¹„)
    raw = re.sub(r"^\s*```[a-zA-Z]*\s*", "", raw)
    raw = re.sub(r"\s*```\s*$", "", raw)

    start = "<!--REPORT_START-->"
    end = "<!--REPORT_END-->"
    if start in raw and end in raw:
        body = raw.split(start, 1)[1].split(end, 1)[0]

        # âœ… ë“¤ì—¬ì“°ê¸° ì œê±°(ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ë°©ì§€)
        lines = [ln.lstrip() for ln in body.splitlines()]
        body = "\n".join(lines).strip()

        return body  # raw HTML

    # ê·¸ ì™¸: ì „ë¶€ escape í›„ <br>ë§Œ ë³µì›
    escaped = html.escape(raw)
    escaped = re.sub(r"&lt;br\s*/?&gt;", "<br>", escaped, flags=re.IGNORECASE)
    return escaped


# endregion


# region [2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (Utilities)]
# ==========================================
def normalize_text(text):
    if not text: return ""
    return re.sub(r'[^a-zA-Z0-9ê°€-í£]', '', text).lower()

PROMPT_FILE_1ST = "1ì°¨ ì§ˆë¬¸ í”„ë¡¬í”„íŠ¸.md"

def extract_report_html(text: str) -> str | None:
    raw = (text or "")
    start = ""
    end = ""
    if start not in raw or end not in raw:
        return None
    body = raw.split(start, 1)[1].split(end, 1)[0]
    body = re.sub(r"```html\s*", "", body, flags=re.IGNORECASE)
    body = re.sub(r"```", "", body)
    if "&lt;" in body and "&gt;" in body:
        body = _html.unescape(body)
    lines = [ln.lstrip() for ln in body.splitlines()]
    body = "\n".join(lines).strip()
    return body if body else None

def render_assistant_content(content: str, css: str = "", height: int = 900):
    raw = (content or "").strip()
    report_html = extract_report_html(raw)
    if report_html is not None:
        st_html(css + report_html, height=height, scrolling=True)
        return
    raw2 = re.sub(r"^\s*```html\s*", "", raw, flags=re.IGNORECASE)
    raw2 = re.sub(r"^\s*```[a-zA-Z]*\s*", "", raw2)
    raw2 = re.sub(r"\s*```\s*$", "", raw2).strip()
    raw2 = "\n".join([ln.lstrip() for ln in raw2.splitlines()]).strip()
    looks_like_html = (
        "<div" in raw2[:500].lower()
        or "<table" in raw2[:500].lower()
        or "class=\"yt-report\"" in raw2[:1500].lower()
        or "class='yt-report'" in raw2[:1500].lower()
    )
    if looks_like_html:
        st_html(css + raw2, height=height, scrolling=True)
        return
    st.markdown(render_md_allow_br(raw), unsafe_allow_html=True)

def load_text_file(filename: str) -> str:
    base_dir = Path(__file__).resolve().parent 
    path = base_dir / filename
    return path.read_text(encoding="utf-8")

def load_prompt_file(filename: str) -> str:
    raw = load_text_file(filename)
    if not raw: return ""
    raw_stripped = raw.strip()
    if raw_stripped.count('"') >= 10 and "\\n" in raw_stripped:
        parts = re.findall(r'"([^"]*)"', raw_stripped)
        if parts:
            merged = "".join(parts)
            merged = merged.replace("\\n", "\n").replace("\\t", "\t")
            merged = merged.replace('\\"', '"')
            return merged.strip()
    return raw_stripped

def format_korean_number(num):
    if num == 0: return "0íšŒ"
    s = ""
    if num >= 100000000:
        eok = num // 100000000
        rem = num % 100000000
        s += f"{int(eok)}ì–µ "
        num = rem
    if num >= 10000:
        man = num // 10000
        rem = num % 10000
        s += f"{int(man)}ë§Œ "
        num = rem
    if num > 0:
        s += f"{int(num)}"
    return s.strip() + "íšŒ"

TRAFFIC_MAP = {
    'YT_SEARCH': 'ìœ íŠœë¸Œ ê²€ìƒ‰', 'RELATED_VIDEO': 'ì¶”ì²œ ë™ì˜ìƒ',
    'BROWSE_FEATURES': 'íƒìƒ‰ ê¸°ëŠ¥', 'EXT_URL': 'ì™¸ë¶€ ë§í¬',
    'NO_LINK_OTHER': 'ê¸°íƒ€', 'PLAYLIST': 'ì¬ìƒëª©ë¡',
    'VIDEO_CARD': 'ì¹´ë“œ/ìµœì¢…í™”ë©´', 'NOTIFICATION': 'ì•Œë¦¼'
}

def map_traffic_source(key):
    return TRAFFIC_MAP.get(key, key)

def parse_utc_to_kst_date(utc_str):
    try:
        dt_utc = datetime.strptime(utc_str, "%Y-%m-%dT%H:%M:%SZ")
        dt_kst = dt_utc + timedelta(hours=9)
        return dt_kst.date()
    except: return None

def parse_duration_to_minutes(duration_str):
    if not duration_str: return 0.0
    pattern = re.compile(r'PT(?:(\d+)H)?(?:(\d+)M)?(?:(\d+)S)?')
    match = pattern.match(duration_str)
    if not match: return 0.0
    h, m, s = match.groups()
    total_sec = (int(h or 0) * 3600) + (int(m or 0) * 60) + (int(s or 0))
    return round(total_sec / 60, 1)

# ==========================================
# [ìˆ˜ì •] Firebase ì €ì¥ + ìºì‹±(25ì‹œê°„) + ì‹œê°„í™•ì¸
# ==========================================
def init_firebase():
    try:
        if not firebase_admin._apps:
            if "firebase" not in st.secrets:
                return None
            cred_dict = dict(st.secrets["firebase"])
            cred = credentials.Certificate(cred_dict)
            firebase_admin.initialize_app(cred)
        return firestore.client()
    except Exception as e:
        print(f"Firebase Init Error: {e}")
        return None

def save_to_firebase(file_name, content_list):
    try:
        db = init_firebase()
        if not db: return False, "Secrets ì„¤ì • ì˜¤ë¥˜ ë˜ëŠ” DB ì—°ê²° ì‹¤íŒ¨"

        doc_ref = db.collection('yt_cache').document(file_name)
        
        old_chunks = doc_ref.collection('chunks').stream()
        for doc in old_chunks:
            doc.reference.delete()

        CHUNK_SIZE = 40 
        total_videos = len(content_list)
        
        doc_ref.set({
            'total_count': total_videos,
            'updated_at': firestore.SERVER_TIMESTAMP # ì„œë²„ ì‹œê°„ ì €ì¥
        })

        batch = db.batch()
        batch_count = 0
        
        for i in range(0, total_videos, CHUNK_SIZE):
            chunk = content_list[i : i + CHUNK_SIZE]
            chunk_index = str(i // CHUNK_SIZE)
            
            chunk_ref = doc_ref.collection('chunks').document(chunk_index)
            batch.set(chunk_ref, {'data': chunk})
            
            batch_count += 1
            
            if batch_count >= 10:
                batch.commit()
                batch = db.batch()
                batch_count = 0
        
        if batch_count > 0:
            batch.commit()
            
        # [ì¤‘ìš”] ì €ì¥ í›„ ìºì‹œ ë¹„ìš°ê¸° (ë‹¤ìŒ í˜¸ì¶œ ì‹œ ìƒˆ ë°ì´í„° ë¡œë”©)
        load_from_firebase.clear()
        get_last_update_time.clear() # ì‹œê°„ ì •ë³´ë„ ê°±ì‹ 
        
        return True, f"Firebase Saved ({total_videos} items)"

    except Exception as e:
        return False, str(e)

# [ì„¤ì •] 25ì‹œê°„ ìœ ì§€ (90000ì´ˆ)
@st.cache_data(ttl=90000, show_spinner=False)
def load_from_firebase(file_name):
    try:
        db = init_firebase()
        if not db: return []

        doc_ref = db.collection('yt_cache').document(file_name)
        main_doc = doc_ref.get()
        if not main_doc.exists:
            return []

        chunks_stream = doc_ref.collection('chunks').stream()
        sorted_chunks = sorted(chunks_stream, key=lambda x: int(x.id) if x.id.isdigit() else x.id)
        
        all_videos = []
        for chunk_doc in sorted_chunks:
            chunk_data = chunk_doc.to_dict().get('data', [])
            all_videos.extend(chunk_data)
            
        return all_videos

    except Exception as e:
        print(f"Load Error: {e}")
        return []

# [ì¶”ê°€] ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ë§Œ ê°€ë³ê²Œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
@st.cache_data(ttl=90000, show_spinner=False)
def get_last_update_time(file_name):
    try:
        db = init_firebase()
        if not db: return None
        doc = db.collection('yt_cache').document(file_name).get()
        if doc.exists:
            data = doc.to_dict()
            if 'updated_at' in data:
                # Firestore Timestampë¥¼ í•œêµ­ ì‹œê°„ ë¬¸ìì—´ë¡œ ë³€í™˜
                ts = data['updated_at']
                dt_utc = ts.replace(tzinfo=None) # naive remove
                dt_kst = dt_utc + timedelta(hours=9)
                return dt_kst.strftime("%Y-%m-%d %H:%M")
        return None
    except:
        return None
# endregion


# region [3. ì‹œê°í™” í•¨ìˆ˜ (Visualization)]
# ==========================================
def get_pyramid_chart_and_df(stats_dict, total_views):
    if not stats_dict: return None, None, ""
    
    age_order = ['age13-17', 'age18-24', 'age25-34', 'age35-44', 'age45-54', 'age55-64', 'age65-']
    display_labels = [label.replace('age', '') for label in age_order]
    
    male_data = defaultdict(float); female_data = defaultdict(float)
    table_rows = []; total_male = 0; total_female = 0

    for key, count in stats_dict.items():
        parts = key.split('_')
        if len(parts) != 2: continue
        age_group, gender = parts[0], parts[1]
        if gender not in ['male', 'female']: continue
        if age_group not in age_order: continue 
        
        pct = (count / total_views) * 100 if total_views > 0 else 0
        clean_age = age_group.replace('age', '')
        
        if gender == 'male':
            male_data[clean_age] += pct; total_male += pct
        elif gender == 'female':
            female_data[clean_age] += pct; total_female += pct
            
        table_rows.append({
            "ì—°ë ¹": clean_age, "ì„±ë³„": "ë‚¨" if gender=='male' else "ì—¬", 
            "ì¡°íšŒìˆ˜": int(count), "ë¹„ìœ¨": pct
        })

    male_vals = [male_data[l] for l in display_labels]
    female_vals = [female_data[l] for l in display_labels]
    male_vals_neg = [-v for v in male_vals] 

    fig = go.Figure()
    fig.add_trace(go.Bar(y=display_labels, x=male_vals_neg, name='ë‚¨ì„±', orientation='h',
        marker=dict(color='#5684D5'), text=[f"{v:.1f}%" if v>0 else "" for v in male_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ë‚¨ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, male_vals)]))
    fig.add_trace(go.Bar(y=display_labels, x=female_vals, name='ì—¬ì„±', orientation='h',
        marker=dict(color='#FF7675'), text=[f"{v:.1f}%" if v>0 else "" for v in female_vals],
        textfont=dict(color='white'), textposition='auto', hoverinfo='text',
        hovertext=[f"ì—¬ì„± {a}: {v:.1f}%" for a, v in zip(display_labels, female_vals)]))
    
    max_val = max(max(male_vals) if male_vals else 0, max(female_vals) if female_vals else 0)
    max_range = max_val * 1.2 if max_val > 0 else 10

    fig.update_layout(
        barmode='overlay',
        xaxis=dict(tickvals=[-max_range, 0, max_range], ticktext=[f"{max_range:.0f}%", "0%", f"{max_range:.0f}%"], range=[-max_range, max_range]),
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        margin=dict(l=10, r=10, t=30, b=10),
        height=300,
        paper_bgcolor='rgba(0,0,0,0)', 
        plot_bgcolor='rgba(0,0,0,0)'
    )
    
    df = pd.DataFrame(table_rows)
    if not df.empty:
        df['ì—°ë ¹'] = pd.Categorical(df['ì—°ë ¹'], categories=display_labels, ordered=True)
        df = df.sort_values(['ì—°ë ¹', 'ì„±ë³„'])

    title_str = f"ğŸ‘¥ ì„±ë³„/ì—°ë ¹ (ë‚¨ {total_male:.1f}% vs ì—¬ {total_female:.1f}%)"
    return fig, df, title_str

def get_traffic_chart(traffic_dict):
    if not traffic_dict: return None
    sorted_t = sorted(traffic_dict.items(), key=lambda x: x[1], reverse=True)
    labels = []; values = []
    for k, v in sorted_t[:5]:
        labels.append(map_traffic_source(k)); values.append(v)
    if len(sorted_t) > 5:
        labels.append("ê¸°íƒ€"); values.append(sum(v for k,v in sorted_t[5:]))
    
    if not values: return None
        
    teal_colors = ['#00b894', '#00cec9', '#55efc4', '#81ecec', '#b2bec3', '#dfe6e9']
    fig = px.pie(names=labels, values=values, hole=0.5, color_discrete_sequence=teal_colors)
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_keyword_bar_chart(keyword_dict):
    if not keyword_dict: return None
    
    sorted_k = sorted(keyword_dict.items(), key=lambda x: x[1], reverse=True)[:10]
    if not sorted_k: return None
    
    words = [k[0] for k in sorted_k][::-1] 
    counts = [k[1] for k in sorted_k][::-1]
    
    fig = go.Figure(go.Bar(
        x=counts, y=words, orientation='h',
        marker=dict(color='#fdcb6e'),
        text=[f"{int(v):,}" for v in counts], textposition='auto'
    ))
    
    fig.update_layout(
        margin=dict(l=10, r=10, t=10, b=10),
        height=300,
        xaxis=dict(showticklabels=False, visible=False),
        yaxis=dict(tickfont=dict(size=13)),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_channel_share_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)
    labels = []; values = []
    top_list = sorted_ch[:5]
    others_sum = sum(ch['total_views'] for ch in sorted_ch[5:])
    
    for ch in top_list:
        labels.append(ch['channel_name']); values.append(ch['total_views'])
    if len(sorted_ch) > 5:
        labels.append("ê·¸ ì™¸ ì±„ë„"); values.append(others_sum)
    
    if sum(values) == 0: return None
        
    colors = []
    if highlight_channel:
        for label in labels:
            colors.append('#6c5ce7' if label == highlight_channel else '#dfe6e9')
    else:
        colors = ['#6c5ce7', '#a29bfe', '#8e44ad', '#9b59b6', '#d6a2e8', '#dfe6e9']

    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=0.5, marker=dict(colors=colors))])
    fig.update_traces(textposition='inside', textinfo='percent+label')
    fig.update_layout(
        margin=dict(l=20, r=20, t=0, b=20), 
        height=300, 
        showlegend=False,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_engagement_chart(ch_details, highlight_channel=None):
    if not ch_details: return None
    sorted_ch = sorted(ch_details, key=lambda x: x['total_views'], reverse=True)[:10]
    names = [c['channel_name'] for c in sorted_ch]
    like_ratios = [(c['total_likes']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    share_ratios = [(c['total_shares']/c['total_views']*100) if c['total_views']>0 else 0 for c in sorted_ch]
    
    if not names: return None

    base_like_color = '#ff7675'; base_share_color = '#74b9ff'
    if highlight_channel:
        like_colors = [base_like_color if n == highlight_channel else '#dfe6e9' for n in names]
        share_colors = [base_share_color if n == highlight_channel else '#dfe6e9' for n in names]
    else:
        like_colors = base_like_color; share_colors = base_share_color

    fig = go.Figure()
    fig.add_trace(go.Bar(x=names, y=like_ratios, name='ì¢‹ì•„ìš”(%)', marker_color=like_colors))
    fig.add_trace(go.Bar(x=names, y=share_ratios, name='ê³µìœ (%)', marker_color=share_colors))

    fig.update_layout(
        barmode='group',
        margin=dict(l=20, r=20, t=10, b=40),
        height=300,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
        yaxis=dict(title="ë¹„ìœ¨(%)", tickformat=".2f"),
        xaxis=dict(tickangle=-45),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_country_map(country_stats):
    if not country_stats: return None
    data = []
    for k, v in country_stats.items():
        iso3 = ISO_MAPPING.get(k, k)
        c_name = k 
        if k == 'KR': c_name = 'ëŒ€í•œë¯¼êµ­'
        elif k == 'US': c_name = 'ë¯¸êµ­'
        elif k == 'JP': c_name = 'ì¼ë³¸'
        elif k == 'VN': c_name = 'ë² íŠ¸ë‚¨'
        elif k == 'TH': c_name = 'íƒœêµ­'
        data.append({'iso_alpha': iso3, 'views': v, 'country': c_name, 'fmt_views': format_korean_number(v)})
    
    if not data: return None
    df_map = pd.DataFrame(data)
    df_kr = df_map[df_map['iso_alpha'] == 'KOR']
    df_others = df_map[df_map['iso_alpha'] != 'KOR']
    
    fig = go.Figure()
    if not df_others.empty:
        fig.add_trace(go.Choropleth(
            locations=df_others['iso_alpha'], z=df_others['views'], text=df_others['country'],
            customdata=df_others[['country', 'fmt_views']], colorscale='Teal',
            marker_line_color='#ffffff', marker_line_width=0.5, showscale=True,
            colorbar=dict(title="ì¡°íšŒìˆ˜", x=1.0, len=0.8),
            hovertemplate="<b>%{customdata[0]}</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))
    if not df_kr.empty:
        fig.add_trace(go.Choropleth(
            locations=df_kr['iso_alpha'], z=[1]*len(df_kr), text=df_kr['country'],
            customdata=df_kr[['country', 'fmt_views']], colorscale=[[0, '#5684D5'], [1, '#5684D5']], 
            marker_line_color='#ffffff', marker_line_width=1, showscale=False,
            hovertemplate="<b>%{customdata[0]} (Home)</b><br>ì¡°íšŒìˆ˜: %{customdata[1]}<extra></extra>"
        ))

    fig.update_geos(
        showcountries=True, countrycolor="#E0E0E0",
        showcoastlines=True, coastlinecolor="#E0E0E0",
        showframe=False, projection_type='natural earth',
        fitbounds="locations" if df_map.empty else False,
        bgcolor='rgba(0,0,0,0)'
    )
    fig.update_layout(
        margin=dict(l=0, r=0, t=0, b=0), 
        height=400, 
        dragmode='pan',
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig

def get_daily_trend_chart(daily_stats, recent_gap=0):
    """
    daily_stats: Analytics API ì¼ë³„ ì¡°íšŒìˆ˜ (ë”•ì…”ë„ˆë¦¬)
    recent_gap: Data API(ì‹¤ì‹œê°„) ì´í•© - Analytics ì´í•©
    """
    if not daily_stats: return None
    
    # 1. ë‚ ì§œìˆœ ì •ë ¬ ë° ë°ì´í„° ì¶”ì¶œ
    sorted_dates = sorted(daily_stats.keys())
    daily_views = [daily_stats[d] for d in sorted_dates]
    
    # 2. ëˆ„ì  ì¡°íšŒìˆ˜(Cumulative) ê³„ì‚°
    cumulative_views = []
    current_sum = 0
    for v in daily_views:
        current_sum += v
        cumulative_views.append(current_sum)
    
    fig = go.Figure()
    
    # [A] í™•ì •ëœ ê³¼ê±° ë°ì´í„° (ì‹¤ì„ )
    fig.add_trace(go.Scatter(
        x=sorted_dates, 
        y=cumulative_views, 
        mode='lines+markers',
        name='ëˆ„ì  ì¡°íšŒìˆ˜ (í™•ì •)',
        line=dict(color='#6c5ce7', width=3),
        marker=dict(size=6),
        hovertemplate='%{x}<br>ëˆ„ì : %{y:,}íšŒ<extra></extra>'
    ))
    
    # [B] ì‹¤ì‹œê°„ êµ¬ê°„ ì—°ê²° (ìµœê·¼ 3ì¼ ì´ë‚´ì¼ ë•Œë§Œ)
    if recent_gap > 0 and sorted_dates:
        last_date_str = sorted_dates[-1]
        last_cum_val = cumulative_views[-1]
        
        today_dt = datetime.today()
        last_anl_dt = datetime.strptime(last_date_str, "%Y-%m-%d")
        
        # ì°¨ì´ê°€ 3ì¼ ì´ë‚´ì¸ ê²½ìš°ì—ë§Œ ì‹¤ì‹œê°„ ì ì„  ì—°ê²°
        if (today_dt - last_anl_dt).days <= 3:
            target_date_str = today_dt.strftime("%Y-%m-%d")
            final_total_val = last_cum_val + recent_gap
            
            if last_date_str != target_date_str:
                fig.add_trace(go.Scatter(
                    x=[last_date_str, target_date_str],
                    y=[last_cum_val, final_total_val],
                    mode='lines+markers',
                    name='ì‹¤ì‹œê°„ ì¶”ì´ (ìµœê·¼)',
                    line=dict(color='#ff7675', width=3, dash='dot'),
                    marker=dict(size=6, symbol='circle-open'),
                    hovertemplate=f'<b>ì‹¤ì‹œê°„(ì¶”ì •)</b><br>í˜„ì¬ ì´í•©: %{{y:,}}íšŒ<br>(ìµœê·¼ +{recent_gap:,}íšŒ ì¦ê°€)<extra></extra>'
                ))
                
                fig.add_annotation(
                    x=target_date_str, y=final_total_val,
                    text=f"Now (+{recent_gap:,})",
                    showarrow=True, arrowhead=2,
                    ax=0, ay=-20,
                    font=dict(color="#d63031", size=11, weight="bold")
                )

    # [Xì¶• ì¤‘ë³µ ë°©ì§€ ë¡œì§]
    # ì¡°íšŒ ê¸°ê°„ì´ ì§§ì„ ë•Œ(ì˜ˆ: 30ì¼ ì´ë‚´)ëŠ” ê°•ì œë¡œ '1ì¼ 1ëˆˆê¸ˆ(D1)'ì„ ì ìš©í•´ ì¤‘ë³µì„ ë§‰ìŠµë‹ˆë‹¤.
    # ê¸°ê°„ì´ ê¸¸ë©´(ì˜ˆ: 1ë…„) ìë™(Auto)ìœ¼ë¡œ ë‘¬ì•¼ ê²¹ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    dtick_setting = None
    if len(sorted_dates) <= 31: 
        dtick_setting = "D1"  # 1ì¼ 1ëˆˆê¸ˆ ê°•ì œ (ì¤‘ë³µ í•´ê²°)

    fig.update_layout(
        title="ğŸ“ˆ ëˆ„ì  ì¡°íšŒìˆ˜ ì„±ì¥ ì¶”ì´",
        margin=dict(l=20, r=20, t=40, b=20),
        height=350, 
        xaxis=dict(
            title=None, 
            tickformat="%m-%d", 
            dtick=dtick_setting  # [ìˆ˜ì •] ì—¬ê¸°ê°€ í•µì‹¬ì…ë‹ˆë‹¤!
        ),
        yaxis=dict(title="ì´ ì¡°íšŒìˆ˜", tickformat=","),
        hovermode="x unified",
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    return fig

def get_efficiency_scatter(video_details):
    if not video_details: return None
    df = pd.DataFrame(video_details)
    if df.empty: return None
    df = df[(df['duration_min'] > 0) & (df['avg_pct'].notnull()) & (df['avg_pct'] > 0)]
    if df.empty: return None
    
    fig = px.scatter(
        df, x='duration_min', y='avg_pct', 
        size='views', color='avg_pct',
        hover_name='title',
        labels={'duration_min': 'ì˜ìƒ ê¸¸ì´(ë¶„)', 'avg_pct': 'í‰ê·  ì§€ì†ë¥ (%)'},
        color_continuous_scale='Viridis'
    )
    fig.add_vline(x=df['duration_min'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    fig.add_hline(y=df['avg_pct'].median(), line_dash="dash", line_color="gray", opacity=0.5)
    
    fig.update_layout(
        margin=dict(l=20, r=20, t=20, b=20),
        height=400,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(showgrid=True, gridcolor='#eee'),
        yaxis=dict(showgrid=True, gridcolor='#eee')
    )
    return fig
# endregion


# region [4. API ë° ë°ì´í„° ì²˜ë¦¬ (API & Data Processing)]
# ==========================================
def get_creds_from_file(token_filename):
    creds = None
    if os.path.exists(token_filename):
        creds = google.oauth2.credentials.Credentials.from_authorized_user_file(token_filename, SCOPES)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            try:
                creds.refresh(google.auth.transport.requests.Request())
                with open(token_filename, 'w') as token: token.write(creds.to_json())
            except: return None
        else: return None
    return creds

def process_sync_channel(token_file, limit_date, status_box, force_rescan):
    # [ë‚´ë¶€í•¨ìˆ˜] DB ë¡œê¹…ìš©
    def log_to_db(level, msg, detail=None):
        try:
            db = init_firebase()
            if db:
                db.collection('system_logs').add({
                    'level': level,
                    'msg': msg,
                    'detail': str(detail),
                    'source': 'process_sync_channel',
                    'timestamp': firestore.SERVER_TIMESTAMP
                })
        except: pass

    # [UI] DummyBox ì²˜ë¦¬
    if status_box is None:
        class DummyBox:
            def success(self, m): pass
            def error(self, m): print(f"[Error] {m}")
            def warning(self, m): pass
            def info(self, m): pass
            def markdown(self, m): pass
        status_box = DummyBox()

    file_label = os.path.basename(token_file).replace("token_", "").replace(".json", "")
    
    # 1. ì¸ì¦ ë° ì±„ë„ ì •ë³´ íšë“
    creds = get_creds_from_file(token_file)
    if not creds: 
        err_msg = f"âŒ [{file_label}] í† í° ì˜¤ë¥˜ (íŒŒì¼ ì½ê¸° ì‹¤íŒ¨)"
        status_box.error(err_msg)
        log_to_db('error', err_msg, token_file)
        return None
        
    try:
        youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
        ch_res = youtube.channels().list(part='snippet,contentDetails', mine=True).execute()
        if not ch_res['items']: 
            status_box.warning(f"âš ï¸ [{file_label}] ì •ë³´ ì—†ìŒ")
            return None
        ch_info = ch_res['items'][0]; ch_name = ch_info['snippet']['title']
        uploads_id = ch_info['contentDetails']['relatedPlaylists']['uploads']
        
        cache_file = f"cache_{token_file}"
        cached_videos = []
        cached_ids = set()
        
        # 2. ë¡œì»¬ ìºì‹œ ë¡œë“œ
        if not force_rescan and os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f: cached_videos = json.load(f)
            cached_ids = {v['id'] for v in cached_videos}
            status_box.info(f"ğŸ”„ [{ch_name}] ë¡œì»¬ ë°ì´í„° í™•ì¸ ({len(cached_videos)}ê°œ)...")
        else: 
            status_box.info(f"â³ [{ch_name}] ìŠ¤ìº” ì‹œì‘")
        
        new_videos = []; next_page_token = None; stop_scanning = False
        
        # 3. ìœ íŠœë¸Œ API ìŠ¤ìº”
        while not stop_scanning:
            req = youtube.playlistItems().list(part='snippet', playlistId=uploads_id, maxResults=50, pageToken=next_page_token)
            res = req.execute()
            
            for item in res['items']:
                vid = item['snippet']['resourceId']['videoId']
                title = item['snippet']['title']
                desc = item['snippet']['description']
                p_at = item['snippet']['publishedAt']
                
                if p_at < limit_date: 
                    stop_scanning = True; break
                
                # [ì¤‘ìš”] ë¡œì»¬ì— ìˆìœ¼ë©´ ìŠ¤ìº”ì€ ë©ˆì¶”ì§€ë§Œ, ë‚˜ì¤‘ì— íŒŒë²  ì €ì¥ì€ ìˆ˜í–‰í•´ì•¼ í•¨
                if not force_rescan and vid in cached_ids: 
                    stop_scanning = True
                    # ì™œ ë©ˆì·„ëŠ”ì§€ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
                    if not new_videos:
                        log_to_db('info', f"[{ch_name}] API ìŠ¤ìº” ì¤‘ë‹¨ (ë¡œì»¬ ë°ì´í„°ì™€ ì¼ì¹˜)", f"ê¸°ì¤€ì˜ìƒ: {title}")
                    break
                
                new_videos.append({'id': vid, 'title': title, 'date': p_at, 'description': desc})
            
            if len(new_videos) > 0 and len(new_videos) % 50 == 0:
                status_box.markdown(f"ğŸƒ **[{ch_name}]** +{len(new_videos)}")
            
            if not res.get('nextPageToken'): stop_scanning = True
            next_page_token = res.get('nextPageToken')
            if not next_page_token: stop_scanning = True
        
        # 4. ë°ì´í„° ë³‘í•©
        if force_rescan:
            preserved_videos = [v for v in cached_videos if v['date'] < limit_date]
            final_list = new_videos + preserved_videos
        else:
            final_list = new_videos + cached_videos
        
        # -----------------------------------------------------------
        # [ìˆ˜ì •ëœ ë¶€ë¶„] ì €ì¥ ë¡œì§ ë¶„ë¦¬
        # -----------------------------------------------------------
        
        # (A) ë¡œì»¬ íŒŒì¼ ì €ì¥ì€ 'ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ' (ë””ìŠ¤í¬ ë³´í˜¸)
        if new_videos or force_rescan:
            with open(cache_file, 'w', encoding='utf-8') as f: 
                json.dump(final_list, f, ensure_ascii=False, indent=2)
                
        # (B) íŒŒì´ì–´ë² ì´ìŠ¤ ì €ì¥ì€ 'í•­ìƒ' ìˆ˜í–‰í•˜ì—¬ ì‹±í¬ ë§ì¶¤
        #     (ìƒˆ ì˜ìƒì´ ì—†ì–´ë„, íŒŒë² ê°€ ë¹„ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°•ì œ ì €ì¥)
        is_ok, msg = save_to_firebase(os.path.basename(cache_file), final_list)
        
        log_msg = f"[{ch_name}] ì²˜ë¦¬ ì™„ë£Œ (ì´ {len(final_list)}ê°œ)"
        
        if is_ok:
            if new_videos:
                status_box.success(f"ğŸ”¥ **[{ch_name}] ì—…ë°ì´íŠ¸ ì™„ë£Œ (+{len(new_videos)})**")
                log_to_db('success', f"[{ch_name}] ì—…ë°ì´íŠ¸ ë° DB ì €ì¥", f"ì¶”ê°€: {len(new_videos)} / ì´: {len(final_list)}")
            else:
                # ë³€ê²½ì‚¬í•­ ì—†ì–´ë„ DB ì €ì¥ì€ ì„±ê³µí–ˆìŒì„ í‘œì‹œ
                status_box.success(f"âœ… **[{ch_name}] ìµœì‹  ìœ ì§€ (DB ë™ê¸°í™” ì™„ë£Œ)**")
                # ë„ˆë¬´ ì¦ì€ ë¡œê·¸ê°€ ì‹«ìœ¼ë©´ ì•„ë˜ ì¤„ ì£¼ì„ ì²˜ë¦¬
                log_to_db('info', f"[{ch_name}] ìµœì‹  ìƒíƒœ ìœ ì§€", f"ì´ {len(final_list)}ê°œ ë™ê¸°í™”")
        else:
            status_box.error(f"âš ï¸ **[{ch_name}]** íŒŒë²  ì €ì¥ ì‹¤íŒ¨:\n{msg}")
            log_to_db('warning', f"[{ch_name}] íŒŒë²  ì €ì¥ ì‹¤íŒ¨", msg)
        
        return {'creds': creds, 'name': ch_name, 'videos': final_list}
        
    except Exception as e:
        status_box.error(f"âŒ ì—ëŸ¬: {str(e)}")
        log_to_db('fatal_error', f"[{file_label}] ë¡œì§ ì—ëŸ¬", str(e))
        return {'error': str(e)}

def process_analysis_channel(channel_data, keyword, vid_start, vid_end, anl_start, anl_end):
    # (ê¸°ì¡´ ë¶„ì„ ë¡œì§ê³¼ ë™ì¼)
    creds = channel_data['creds']; videos = channel_data['videos']
    norm_keyword = normalize_text(keyword)
    target_ids = []
    id_map = {} 
    video_date_map = {}
    
    # 1. ì˜ìƒ í•„í„°ë§
    for v in videos:
        t_match = norm_keyword in normalize_text(v['title'])
        d_match = norm_keyword in normalize_text(v.get('description', ''))
        if not (t_match or d_match): continue
        
        v_dt_kst = parse_utc_to_kst_date(v['date'])
        if v_dt_kst and (vid_start <= v_dt_kst <= vid_end): 
            target_ids.append(v['id'])
            id_map[v['id']] = v['title']
            video_date_map[v['id']] = v['date']
            
    if not target_ids: return None
    
    yt_anl = googleapiclient.discovery.build('youtubeAnalytics', 'v2', credentials=creds)
    youtube = googleapiclient.discovery.build('youtube', 'v3', credentials=creds)
    
    total_views = 0; total_likes = 0; total_shares = 0
    demo = defaultdict(float); traffic = defaultdict(float)
    country = defaultdict(float); daily = defaultdict(float)
    keywords_count = defaultdict(float)
    w_avg_sum = 0; v_for_avg = 0
    over_1m_count = 0 
    
    top_video_stats = []
    
    today_date = datetime.now().date()
    
    if isinstance(anl_end, str):
        anl_end_date = datetime.strptime(anl_end, "%Y-%m-%d").date()
    else:
        anl_end_date = anl_end

    if isinstance(anl_start, str):
        anl_start_date = datetime.strptime(anl_start, "%Y-%m-%d").date()
    else:
        anl_start_date = anl_start
        
    use_hybrid_logic = anl_end_date >= (today_date - timedelta(days=2))
    
    batch_size = 50 
    for i in range(0, len(target_ids), batch_size):
        batch_ids = target_ids[i : i + batch_size]
        vid_str = ",".join(batch_ids)
        
        anl_views_map = {}; anl_likes_map = {}; anl_retention_map = {}
        
        try:
            r_v = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views,likes,averageViewPercentage', dimensions='video', filters=f'video=={vid_str}').execute()
            if 'rows' in r_v and r_v['rows']:
                for r in r_v['rows']:
                    anl_views_map[r[0]] = r[1]; anl_likes_map[r[0]] = r[2]; anl_retention_map[r[0]] = r[3]
            
            r_b = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='shares', filters=f'video=={vid_str}').execute()
            if 'rows' in r_b and r_b['rows']: total_shares += r_b['rows'][0][0]

            r_d = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='viewerPercentage', dimensions='ageGroup,gender', filters=f'video=={vid_str}').execute()
            batch_total_view_anl = sum(anl_views_map.values())
            if 'rows' in r_d and r_d['rows']:
                for r in r_d['rows']: demo[f"{r[0]}_{r[1]}"] += batch_total_view_anl * (r[2] / 100)

            r_t = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceType', filters=f'video=={vid_str}').execute()
            if 'rows' in r_t and r_t['rows']:
                for r in r_t['rows']: traffic[r[0]] += r[1]
            
            try:
                r_k = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='insightTrafficSourceDetail', filters=f'video=={vid_str};insightTrafficSourceType==YT_SEARCH', maxResults=15, sort='-views').execute()
                if 'rows' in r_k and r_k['rows']:
                    for r in r_k['rows']:
                        if r[0] != 'GOOGLE_SEARCH': keywords_count[r[0]] += r[1]
            except: pass

            r_c = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='country', filters=f'video=={vid_str}', maxResults=50).execute()
            if 'rows' in r_c and r_c['rows']:
                for r in r_c['rows']: country[r[0]] += r[1]

            r_day = yt_anl.reports().query(ids='channel==MINE', startDate=anl_start, endDate=anl_end, metrics='views', dimensions='day', filters=f'video=={vid_str}', sort='day').execute()
            if 'rows' in r_day and r_day['rows']:
                for r in r_day['rows']: daily[r[0]] += r[1]
        except: pass

        try:
            rt_res = youtube.videos().list(part='statistics,contentDetails', id=vid_str).execute()
            rt_stats_map = {}; rt_content_map = {}
            if 'items' in rt_res:
                for item in rt_res['items']:
                    rt_stats_map[item['id']] = item['statistics']
                    rt_content_map[item['id']] = item['contentDetails']

            for vid_id in batch_ids:
                v_date_str = video_date_map.get(vid_id)
                if not v_date_str: continue
                v_upload_dt = parse_utc_to_kst_date(v_date_str)
                if isinstance(v_upload_dt, datetime): v_upload_dt = v_upload_dt.date()

                a_v = anl_views_map.get(vid_id, 0)
                a_l = anl_likes_map.get(vid_id, 0)
                a_pct = anl_retention_map.get(vid_id, 0)
                
                stats = rt_stats_map.get(vid_id, {})
                rt_v = int(stats.get('viewCount', 0))
                rt_l = int(stats.get('likeCount', 0))
                
                final_v = 0; final_l = 0
                
                if use_hybrid_logic:
                    if v_upload_dt >= anl_start_date:
                        final_v = rt_v; final_l = rt_l
                    else:
                        deduct_end_date = anl_start_date - timedelta(days=1)
                        deduct_start_str = v_upload_dt.strftime("%Y-%m-%d")
                        deduct_end_str = deduct_end_date.strftime("%Y-%m-%d")
                        
                        past_views = 0; past_likes = 0; is_past_data_fetched = False
                        try:
                            if v_upload_dt <= deduct_end_date:
                                r_past = yt_anl.reports().query(ids='channel==MINE', startDate=deduct_start_str, endDate=deduct_end_str, metrics='views,likes', filters=f'video=={vid_id}').execute()
                                if 'rows' in r_past and r_past['rows']:
                                    past_views = r_past['rows'][0][0]
                                    past_likes = r_past['rows'][0][1]
                                    is_past_data_fetched = True
                                else:
                                    past_views = 0; past_likes = 0; is_past_data_fetched = True
                        except: is_past_data_fetched = False

                        if is_past_data_fetched:
                            final_v = max(0, rt_v - past_views)
                            final_l = max(0, rt_l - past_likes)
                        else:
                            final_v = a_v; final_l = a_l

                        if final_v == rt_v and a_v > 0 and past_views == 0:
                             final_v = a_v; final_l = a_l
                else:
                    final_v = a_v; final_l = a_l

                if final_v == 0 and rt_v > 0 and use_hybrid_logic:
                    final_v = rt_v; final_l = rt_l
                
                total_views += final_v; total_likes += final_l
                
                if final_v > 0 and a_pct > 0:
                    w_avg_sum += (final_v * a_pct)
                    v_for_avg += final_v
                
                if rt_v >= 1000000: over_1m_count += 1

                if final_v > 0:
                    top_video_stats.append({
                        'id': vid_id, 'title': id_map.get(vid_id, 'Unknown'),
                        'views': rt_v, 'likes': rt_l, 'period_views': final_v, 'period_likes': final_l,
                        'avg_pct': a_pct if a_pct > 0 else None,
                        'duration_min': parse_duration_to_minutes(rt_content_map.get(vid_id, {}).get('duration'))
                    })

        except: pass
        time.sleep(0.05)

    if not top_video_stats and total_views == 0: return None
    top_video_stats.sort(key=lambda x: x['period_views'], reverse=True)

    return {
        'channel_name': channel_data['name'], 'video_count': len(target_ids),
        'total_views': total_views, 'total_likes': total_likes, 'total_shares': total_shares,
        'avg_view_pct': (w_avg_sum/v_for_avg) if v_for_avg > 0 else 0,
        'demo_counts': demo, 'traffic_counts': traffic,
        'country_counts': country, 'daily_stats': daily,
        'keywords_counts': keywords_count,
        'top_video_stats': top_video_stats,
        'over_1m_count': over_1m_count 
    }

# ==========================================
# [ì¶”ê°€] ìë™ ìŠ¤ì¼€ì¤„ëŸ¬ (ë§¤ì¼ ì•„ì¹¨ 9ì‹œ)
# ==========================================
def job_auto_update_data():
    print(f"â° [Auto-Update] ìë™ ìˆ˜ì§‘ ì‹œì‘: {datetime.now()}")
    
    # DB ì—°ê²° (ì‹œìŠ¤í…œ ë¡œê·¸ìš©)
    db = init_firebase() 
    
    token_files = glob.glob("token_*.json")
    if not token_files:
        msg = "âŒ [Auto] í† í° íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤."
        print(msg)
        if db: db.collection('system_logs').add({'level': 'error', 'msg': msg, 'time': firestore.SERVER_TIMESTAMP})
        return

    try:
        success_cnt = 0
        for tf in token_files:
            # ìœ„ì—ì„œ ìˆ˜ì •í•œ í•¨ìˆ˜ê°€ í˜¸ì¶œë˜ë©´ì„œ ë‚´ë¶€ì ìœ¼ë¡œ ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
            res = process_sync_channel(tf, DEFAULT_LIMIT_DATE, None, False)
            if res and 'error' not in res:
                success_cnt += 1
            print(f"âœ… [Auto-Update] ì²˜ë¦¬ ì‹œë„: {tf}")
        
        # ìºì‹œ ì´ˆê¸°í™”
        load_from_firebase.clear()
        get_last_update_time.clear()
        
        # ìµœì¢… ì™„ë£Œ ë¡œê·¸
        if db:
            db.collection('system_logs').add({
                'level': 'info',
                'msg': f"â° [Auto] ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ ì™„ë£Œ",
                'detail': f"ì‹œë„: {len(token_files)}ê°œ / ì„±ê³µë°˜í™˜: {success_cnt}ê°œ",
                'time': firestore.SERVER_TIMESTAMP
            })
            
    except Exception as e:
        err_msg = f"âš ï¸ [Auto-Update] ìŠ¤ì¼€ì¤„ëŸ¬ ë©ˆì¶¤ (Crash): {e}"
        print(err_msg)
        if db:
            db.collection('system_logs').add({
                'level': 'fatal_error',
                'msg': err_msg,
                'time': firestore.SERVER_TIMESTAMP
            })

@st.cache_resource
def init_scheduler():
    scheduler = BackgroundScheduler()
    korea_tz = pytz.timezone('Asia/Seoul')
    trigger = CronTrigger(hour=9, minute=0, timezone=korea_tz)
    scheduler.add_job(job_auto_update_data, trigger)
    scheduler.start()
    print("ğŸš€ [Scheduler] ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. (ë§¤ì¼ 09:00 KST)")

init_scheduler()
# endregion


# region [5. ë©”ì¸ UI ë° ì‹¤í–‰ ë¡œì§ (Main UI & Execution)]
# ==========================================
st.title("ğŸ“Š Drama YouTube Insight")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.header("ğŸ›ï¸ ë°ì´í„° ê´€ë¦¬ ì„¼í„°")
    
    # [ë³´ì•ˆ] ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ ë¡œì§
    if 'admin_auth' not in st.session_state: st.session_state['admin_auth'] = False
    
    if not st.session_state['admin_auth']:
        st.caption("ğŸ”’ ê´€ê³„ì ì „ìš© ë©”ë‰´ì…ë‹ˆë‹¤.")
        admin_pw = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password", key="sidebar_pw")
        if admin_pw:
            # secrets.toml ì˜ [admin] ì„¹ì…˜ì—ì„œ ë¹„ë°€ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸°
            correct_pw = st.secrets.get("admin", {}).get("password", "")
            if admin_pw == correct_pw:
                st.session_state['admin_auth'] = True
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ ë¶ˆì¼ì¹˜")
    
    # [ë³´ì•ˆ í†µê³¼ í›„] ë©”ë‰´ í‘œì‹œ
    if st.session_state['admin_auth']:
        token_files = glob.glob("token_*.json")
        st.markdown("---")
        
        # [ì¶”ê°€] ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í‘œì‹œ (í† í° íŒŒì¼ ì¤‘ ì²« ë²ˆì§¸ ê¸°ì¤€)
        if token_files:
             last_time_str = get_last_update_time(f"cache_{os.path.basename(token_files[0])}")
             if last_time_str:
                 st.info(f"ğŸ•’ ìµœê·¼ ì—…ë°ì´íŠ¸: {last_time_str}")
             else:
                 st.caption("ì—…ë°ì´íŠ¸ ê¸°ë¡ ì—†ìŒ")
        
        st.caption("ë°ì´í„° ë™ê¸°í™”")
        if st.button("ğŸ”„ ìµœì‹  ì˜ìƒ ì—…ë°ì´íŠ¸ (ìˆ˜ë™)", type="primary", use_container_width=True):
            if not token_files: st.error("ì—°ë™ëœ í† í° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.session_state['channels_data'] = []
                st.write("--- ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘ ---")
                placeholders = {tf: st.empty() for tf in token_files}
                ready = []
                ctx = get_script_run_ctx()
                def sync_worker(tf, sb):
                    add_script_run_ctx(ctx=ctx)
                    return process_sync_channel(tf, DEFAULT_LIMIT_DATE, sb, False)
                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                    futs = {ex.submit(sync_worker, tf, placeholders[tf]): tf for tf in token_files}
                    for f in as_completed(futs):
                        res = f.result()
                        if res and 'name' in res: ready.append(res)
                st.session_state['channels_data'] = ready
                if ready: 
                    st.success("ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                    # ì‹œê°„ ê°±ì‹ ì„ ìœ„í•´ ìºì‹œ ì‚­ì œ í›„ ë¦¬ëŸ°
                    load_from_firebase.clear()
                    get_last_update_time.clear()
                    time.sleep(1)
                    st.rerun()
                
        st.markdown("---")
        
        # [ë³€ê²½] ë©”ë‰´ ì´ë¦„ ìˆ˜ì • (í—·ê°ˆë¦¼ ë°©ì§€)
        with st.expander("âš ï¸ DB ì´ˆê¸°í™” ë° ì „ì²´ ì¬ìˆ˜ì§‘ (Admin)"):
            if 'admin_unlocked' not in st.session_state: st.session_state['admin_unlocked'] = False
            
            # ì—¬ê¸°ì„œë„ ì´ì¤‘ ì ê¸ˆ (ê¸°ì¡´ ìœ ì§€)
            if not st.session_state['admin_unlocked']:
                st.caption("ì •ë§ ì „ì²´ ë°ì´í„°ë¥¼ ê°ˆì•„ì—ìœ¼ì‹œê² ìŠµë‹ˆê¹Œ?")
                if st.text_input("2ì°¨ ë¹„ë°€ë²ˆí˜¸", type="password", key="pw_input") == "dima1234":
                    st.session_state['admin_unlocked'] = True
                    st.rerun()
                    
            if st.session_state['admin_unlocked']:
                st.error("ğŸš¨ ì£¼ì˜: ë§¤ìš° ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.")
                l_date = st.date_input("ìˆ˜ì§‘ ë§ˆì§€ë…¸ì„ ", value=pd.to_datetime(DEFAULT_LIMIT_DATE))
                if st.button("ğŸ”¥ ì „ì²´ ë°ì´í„° ë®ì–´ì“°ê¸° (ì‹¤í–‰)", type="secondary"):
                    st.session_state['channels_data'] = []
                    placeholders = {tf: st.empty() for tf in token_files}
                    ready = []
                    ctx = get_script_run_ctx()
                    def deep_worker(tf, sb):
                        add_script_run_ctx(ctx=ctx)
                        return process_sync_channel(tf, l_date.strftime("%Y-%m-%d"), sb, True)
                    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                        futs = {ex.submit(deep_worker, tf, placeholders[tf]): tf for tf in token_files}
                        for f in as_completed(futs):
                            res = f.result()
                            if res and 'name' in res: ready.append(res)
                    st.session_state['channels_data'] = ready
                    if ready: 
                        st.success("ì™„ë£Œ!")
                        load_from_firebase.clear()
                        get_last_update_time.clear()
                        time.sleep(1)
                        st.rerun()
                if st.button("ğŸ”’ ì ê¸ˆ"):
                    st.session_state['admin_unlocked'] = False
                    st.rerun()
    
    # [ì¶”ê°€] ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ (ì„ íƒì‚¬í•­)
    if st.session_state['admin_auth']:
         if st.button("ë¡œê·¸ì•„ì›ƒ", use_container_width=True):
             st.session_state['admin_auth'] = False
             st.rerun()

# --- ë©”ì¸ ---
if 'channels_data' not in st.session_state or not st.session_state['channels_data']:
    token_files = glob.glob("token_*.json")
    temp_data = []
    
    # [ë³€ê²½] ë¡œì»¬ íŒŒì¼ ëŒ€ì‹  Firebase(ìºì‹œ)ì—ì„œ ë¨¼ì € ë¡œë“œ ì‹œë„
    # í† í° íŒŒì¼ ì´ë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ Firebase ìºì‹œ íŒŒì¼ëª… ì¶”ë¡ 
    for tf in token_files:
        cache_name = f"cache_{os.path.basename(tf)}"
        
        # 1. Firebaseì—ì„œ ê°€ì ¸ì˜¤ê¸° (ìºì‹± ì ìš©ë¨)
        vids = load_from_firebase(cache_name)
        
        if vids:
             creds = get_creds_from_file(tf)
             if creds:
                 lbl = os.path.basename(tf).replace("token_", "").replace(".json", "")
                 temp_data.append({'creds': creds, 'name': lbl, 'videos': vids})
        else:
            # 2. Firebase ì‹¤íŒ¨ ì‹œ ë¡œì»¬ ë°±ì—… í™•ì¸ (ê¸°ì¡´ ë¡œì§)
            if os.path.exists(cache_name):
                 with open(cache_name, 'r', encoding='utf-8') as f:
                    try:
                        vids = json.load(f); creds = get_creds_from_file(tf)
                        if creds:
                            lbl = os.path.basename(tf).replace("token_", "").replace(".json", "")
                            temp_data.append({'creds': creds, 'name': lbl, 'videos': vids})
                    except: pass
                    
    if temp_data: st.session_state['channels_data'] = temp_data
    else: st.info("ğŸ‘‹ ë°ì´í„° ì¤€ë¹„ ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” (ìë™ ìˆ˜ì§‘ ëŒ€ê¸° ì¤‘)")

if 'channels_data' in st.session_state and st.session_state['channels_data']:
    data = st.session_state['channels_data']
    tv = sum(len(c['videos']) for c in data)
    
    st.markdown(f"""
    <div style='background-color:white; padding:10px 20px; border-radius:8px; border:1px solid #eee; margin-bottom:20px; display:flex; align-items:center; gap:10px;'>
        <span>âœ… <b>ì—°ë™ ìƒíƒœ:</b> ì±„ë„ <span style='color:#2980b9; font-weight:bold'>{len(data)}ê°œ</span></span>
        <span style='color:#ddd'>|</span>
        <span>ğŸ“ <b>DB ì˜ìƒ:</b> <span style='color:#2980b9; font-weight:bold'>{tv:,}ê°œ</span></span>
    </div>
    """, unsafe_allow_html=True)

    today = datetime.today()
    first_day = today.replace(day=1)

    with st.form("analysis_form"):
        st.subheader("ğŸ” í†µí•© ë¶„ì„ ì„¤ì •")
        c1, c2, c3 = st.columns([2, 1, 1])
        with c1: keyword = st.text_input("ë¶„ì„ IP", placeholder="ì˜ˆ: ëˆˆë¬¼ì˜ ì—¬ì™•")
        with c2: v_dates = st.date_input("ì˜ìƒ ì—…ë¡œë“œ ê¸°ê°„", value=(first_day, today))
        with c3: a_dates = st.date_input("ë°ì´í„° ì‚°ì¶œ ê¸°ê°„", value=(first_day, today))
        submit_btn = st.form_submit_button("ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

    if submit_btn:
        if isinstance(v_dates, tuple):
            v_start = v_dates[0]; v_end = v_dates[1] if len(v_dates)>1 else v_dates[0]
        else: v_start = v_end = v_dates
        if isinstance(a_dates, tuple):
            a_start = a_dates[0]; a_end = a_dates[1] if len(a_dates)>1 else a_dates[0]
        else: a_start = a_end = a_dates

        if not keyword.strip(): st.error("âš ï¸ ë¶„ì„ IPë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            st.session_state["chat_active"] = False
            st.session_state["chat_history"] = []
            st.session_state["chat_context_comments"] = ""

            vs_str, ve_str = v_start.strftime("%Y-%m-%d"), v_end.strftime("%Y-%m-%d")
            st.session_state['analysis_dates'] = {'start': vs_str, 'end': ve_str}
            
            as_str = a_start.strftime("%Y-%m-%d"); ae_str = a_end.strftime("%Y-%m-%d")
            
            prog_bar = st.progress(0, text="ë°ì´í„° ë¶„ì„ ì¤‘...")
            ch_details_results = []
            
            ctx = get_script_run_ctx()
            def worker(cd, kw, vs, ve, ast, aet):
                add_script_run_ctx(ctx=ctx)
                return process_analysis_channel(cd, kw, vs, ve, ast, aet)

            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
                futures = {ex.submit(worker, ch, keyword, v_start, v_end, as_str, ae_str): ch for ch in data}
                done = 0
                for f in as_completed(futures):
                    done += 1
                    prog_bar.progress(done/len(data), text=f"ì±„ë„ ë¶„ì„ ì¤‘... ({done}/{len(data)})")
                    res = f.result()
                    if res: ch_details_results.append(res)

            prog_bar.empty()
            
            if ch_details_results:
                st.session_state['analysis_raw_results'] = ch_details_results
                st.session_state['analysis_keyword'] = keyword
                st.success(f"ë¶„ì„ ì™„ë£Œ! ì´ {len(ch_details_results)}ê°œ ì±„ë„ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            else:
                st.session_state['analysis_raw_results'] = []
                st.warning(f"âš ï¸ '{keyword}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
                           f"1. **ì˜ìƒ ì—…ë¡œë“œ ê¸°ê°„**ì´ ì´ˆê¸°í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ì„¸ìš”. (í˜„ì¬ ì„¤ì •: {vs_str} ~ {ve_str})\n"
                           f"2. í‚¤ì›Œë“œê°€ ì •í™•í•œì§€ í™•ì¸í•´ë³´ì„¸ìš”.")

    if 'analysis_raw_results' in st.session_state and st.session_state['analysis_raw_results']:
        raw_data = st.session_state['analysis_raw_results']
        current_kw = st.session_state['analysis_keyword']
        
        st.divider()
        st.markdown(f"### ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸: <span style='color:#2980b9;'>{current_kw}</span>", unsafe_allow_html=True)
        
        ch_names = sorted([d['channel_name'] for d in raw_data])
        sel_options = ["ì „ì²´ ì±„ë„ í•©ì‚°"] + ch_names
        
        c_sel_col, _ = st.columns([1, 2])
        with c_sel_col:
            selected_ch = st.selectbox("ë¶„ì„ ëŒ€ìƒ ì±„ë„ ì„ íƒ", sel_options, label_visibility="collapsed")
        
        if selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°":
            target_data = raw_data
        else:
            target_data = [d for d in raw_data if d['channel_name'] == selected_ch]
            
        final_views = 0; final_likes = 0; final_shares = 0
        final_over_1m = 0; final_vid_count = 0
        final_stats = defaultdict(float); final_traffic = defaultdict(float)
        final_country = defaultdict(float); final_daily = defaultdict(float)
        final_keywords = defaultdict(float)
        w_avg_sum = 0; v_for_avg = 0
        final_top_videos = []
        
        for d in target_data:
            final_views += d['total_views']
            final_likes += d['total_likes']
            final_shares += d['total_shares']
            final_over_1m += d['over_1m_count']
            final_vid_count += d['video_count']
            
            if d['avg_view_pct'] > 0 and d['total_views'] > 0:
                w_avg_sum += (d['avg_view_pct'] * d['total_views'])
                v_for_avg += d['total_views']
            
            for k, v in d['demo_counts'].items(): final_stats[k] += v
            for k, v in d['traffic_counts'].items(): final_traffic[k] += v
            for k, v in d['country_counts'].items(): final_country[k] += v
            for k, v in d['daily_stats'].items(): final_daily[k] += v
            for k, v in d['keywords_counts'].items(): final_keywords[k] += v
            if 'top_video_stats' in d: final_top_videos.extend(d['top_video_stats'])
            
        final_avg_pct = (w_avg_sum / v_for_avg) if v_for_avg > 0 else 0
        
        anl_total_daily = sum(final_daily.values())
        recent_gap = final_views - anl_total_daily
        
        safe_anl_date = datetime.now() - timedelta(days=3)
        safe_str = safe_anl_date.strftime("%Y-%m-%d")
        
        if final_views > 0 or len(final_top_videos) > 0:
            st.caption(f"â„¹ï¸ **ë°ì´í„° ê¸°ì¤€**: ì¸êµ¬í†µê³„/ê²½ë¡œ ë“±ì€ **~{safe_str}** í™•ì •ì¹˜, **ì´ ì¡°íšŒìˆ˜/ì¢‹ì•„ìš” ë° ë¦¬ìŠ¤íŠ¸**ëŠ” **ì‹¤ì‹œê°„(Realtime)** ë°ì´í„°ì…ë‹ˆë‹¤.")

            m1, m2, m3, m4, m5, m6 = st.columns(6)
            m1.metric("ì´ ì¡°íšŒìˆ˜", f"{int(final_views):,}")
            m2.metric("ë¶„ì„ ì˜ìƒ", f"{final_vid_count:,}ê°œ")
            m3.metric("100ë§Œ+ ì˜ìƒ", f"{final_over_1m:,}ê°œ")
            m4.metric("í‰ê·  ì‹œì²­ì§€ì†ë¥ ", f"{final_avg_pct:.1f}%")
            m5.metric("ì´ ì¢‹ì•„ìš”", f"{int(final_likes):,}")
            m6.metric("ì´ ê³µìœ ", f"{int(final_shares):,}")
            st.write("")

            fig_demo, df_table, _ = get_pyramid_chart_and_df(final_stats, final_views)
            if fig_demo:
                c1, c2 = st.columns([1.6, 1])
                with c1:
                    st.markdown("##### ğŸ‘¥ ì„±ë³„/ì—°ë ¹ ë¶„í¬")
                    with st.container(border=True):
                        st.plotly_chart(fig_demo, use_container_width=True)
                with c2:
                    st.markdown("##### ğŸ“‹ ìƒì„¸ ë°ì´í„°")
                    with st.container(border=True):
                        if not df_table.empty:
                            df_disp = df_table.copy()
                            df_disp['ì¡°íšŒìˆ˜'] = df_disp['ì¡°íšŒìˆ˜'].apply(lambda x: f"{x:,}")
                            df_disp['ë¹„ìœ¨'] = df_disp['ë¹„ìœ¨'].apply(lambda x: f"{x:.1f}%")
                            st.dataframe(df_disp, use_container_width=True, hide_index=True, height=300)
                st.write("")

            fig_trend = get_daily_trend_chart(final_daily, recent_gap)
            if fig_trend:
                st.markdown("##### ğŸ“ˆ ì¼ë³„ ì¡°íšŒìˆ˜ ì¶”ì´")
                with st.container(border=True):
                    st.plotly_chart(fig_trend, use_container_width=True)
                st.write("")

            st.markdown("##### ğŸ¥‡ ì¸ê¸° ì˜ìƒ TOP 100 (ê¸°ê°„ ë‚´ ì„±ê³¼ ê¸°ì¤€)")
            with st.container(border=True):
                if final_top_videos:
                    unique_vids_map = {v['id']: v for v in final_top_videos}
                    deduped_vids = list(unique_vids_map.values())
                    
                    top_vids = sorted(deduped_vids, key=lambda x: x['period_views'], reverse=True)[:100]
                    
                    df_top = pd.DataFrame(top_vids)
                    df_top['link'] = df_top['id'].apply(lambda x: f"[https://youtu.be/](https://youtu.be/){x}")
                    
                    df_show = df_top[['title', 'period_views', 'avg_pct', 'period_likes', 'link']].copy()
                    df_show.columns = ['ì œëª©', 'ì¡°íšŒìˆ˜', 'ì§€ì†ë¥ (%)', 'ì¢‹ì•„ìš”', 'ë°”ë¡œê°€ê¸°']
                    
                    df_show['ì¡°íšŒìˆ˜'] = df_show['ì¡°íšŒìˆ˜'].apply(lambda x: f"{int(x):,}")
                    df_show['ì¢‹ì•„ìš”'] = df_show['ì¢‹ì•„ìš”'].apply(lambda x: f"{int(x):,}")
                    
                    st.data_editor(
                        df_show,
                        column_config={
                            "ë°”ë¡œê°€ê¸°": st.column_config.LinkColumn(display_text="Watch ğŸ¬"),
                            "ì§€ì†ë¥ (%)": st.column_config.NumberColumn(format="%.1f%%"),
                        },
                        hide_index=True, use_container_width=True, disabled=True
                    )
                else: st.caption("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.write("")

            fig_traffic = get_traffic_chart(final_traffic)
            fig_keywords = get_keyword_bar_chart(final_keywords)
            
            if fig_traffic or fig_keywords:
                r2_1, r2_2 = st.columns(2)
                with r2_1:
                    if fig_traffic:
                        st.markdown("##### ğŸš¦ ìœ ì… ê²½ë¡œ Top 5")
                        with st.container(border=True):
                            st.plotly_chart(fig_traffic, use_container_width=True)
                with r2_2:
                    if fig_keywords:
                        st.markdown("##### ğŸ” Top 10 ê²€ìƒ‰ì–´ (SEO)")
                        with st.container(border=True):
                            st.plotly_chart(fig_keywords, use_container_width=True)
                st.write("")
            
            show_share = (selected_ch == "ì „ì²´ ì±„ë„ í•©ì‚°") and (len(raw_data) > 1)
            fig_share = get_channel_share_chart(raw_data, highlight_channel=None) if show_share else None
            if selected_ch != "ì „ì²´ ì±„ë„ í•©ì‚°" and len(raw_data) > 1:
                fig_share = get_channel_share_chart(raw_data, highlight_channel=selected_ch)
            
            fig_engage = get_engagement_chart(target_data, highlight_channel=selected_ch if selected_ch!="ì „ì²´ ì±„ë„ í•©ì‚°" else None)

            if fig_share or fig_engage:
                r3_1, r3_2 = st.columns(2)
                with r3_1:
                    if fig_share:
                        st.markdown("##### ğŸ† ì±„ë„ë³„ ì ìœ ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_share, use_container_width=True)
                with r3_2:
                    if fig_engage:
                        st.markdown("##### â¤ï¸ ì¢‹ì•„ìš”/ê³µìœ  ë¹„ìœ¨")
                        with st.container(border=True):
                            st.plotly_chart(fig_engage, use_container_width=True)
                st.write("")

            fig_map = get_country_map(final_country)
            if fig_map:
                c_map1, c_map2 = st.columns(2)
                with c_map1:
                    st.markdown("##### ğŸŒ ê¸€ë¡œë²Œ ì¡°íšŒìˆ˜ ë¶„í¬")
                    with st.container(border=True):
                        st.plotly_chart(fig_map, use_container_width=True)
                st.write("")

        else:
            st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
# endregion
